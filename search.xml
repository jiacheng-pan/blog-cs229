<?xml version="1.0" encoding="utf-8"?>
<search> 
  
    
    <entry>
      <title>13. SVM（四）非线性决策边界</title>
      <link href="/blog-cs229/2018/07/25/13.%20SVM%EF%BC%88%E5%9B%9B%EF%BC%89%E9%9D%9E%E7%BA%BF%E6%80%A7%E5%86%B3%E7%AD%96%E8%BE%B9%E7%95%8C/"/>
      <url>/blog-cs229/2018/07/25/13.%20SVM%EF%BC%88%E5%9B%9B%EF%BC%89%E9%9D%9E%E7%BA%BF%E6%80%A7%E5%86%B3%E7%AD%96%E8%BE%B9%E7%95%8C/</url>
      <content type="html"><![CDATA[<p><img src="http://jackie-image.oss-cn-hangzhou.aliyuncs.com/18-7-25/83102162.jpg" alt=""></p><p>当数据中存在异常点时，比如上述的情况，导致原先可以用直线a分割的数据现在不得不用b来进行，以保证完美的分割。由此我们引出了<strong>非线性决策边界</strong>（<em>non-linear decision boundaries</em>）来解决这样的问题。</p><p>观察原SVM问题的目标：<br>$$<br>\min_{w, b} \frac{1}{2}||w|^2 \\<br>\text{ s.t. }y^{(i)} \cdot (w^T \cdot x^{(i)}+b) \geq 1, i=1,\ldots,m<br>$$<br>我们为原公式增加惩罚项，对不同的数据点增加不同的惩罚，使得所有样本能够更好地分割：<br>$$<br>\min_{w, b} \frac{1}{2}||w|^2+c\sum_{i=1}^m\xi_i, \xi_i\geq0<br>$$<br>使得<br>$$<br>y^{(i)} \cdot (w^T \cdot x^{(i)}+b) \geq 1-\xi_i, i=1,\ldots,m<br>$$<br>注意到，我们之前认为$ y^{(i)} \cdot (w^T \cdot x^{(i)}+b) \geq 1 $是分类正确的，在这里我们允许一部分样本小于1，也就是说明我们允许了一部分样本分类错误。</p><p>构建拉格朗日算子：<br>$$<br>{\cal L}(w, b, \xi, \alpha, \gamma) = \frac{1}{2}||w|^2+c\sum_i\xi_i-\sum_i^m\alpha_i(y^{(i)} (w^T \cdot x^{(i)}+b)-1+\xi_i)-\sum_i^m\gamma_i\xi_i<br>$$<br>对偶：<br>$$<br>\max W(\alpha) = \sum_{i=1}\alpha_i-\frac{1}{2}\sum_{i=1}\sum_{j=1}\alpha_i\alpha_jy_iy_j\langle x_i \cdot x_j \rangle<br>$$<br>跟原先的SVM问题的唯一区别在于其限制条件为：<br>$$<br>\sum_{i=1}^my^{(i)}\alpha_i=0 \\<br>0 \leq \alpha_i \leq c<br>$$<br>其收敛条件：</p><ul><li><p>对于大部分数据点：</p><p>$$<br>\alpha_i=0 \Rightarrow y^{(i)} (w^T \cdot x^{(i)}+b) \geq 1<br>$$</p></li><li><p>对于异常点：</p><p>$$<br>\alpha_i = c<br>$$</p></li><li><p>对于最近点：</p><p>$$<br>0&lt;\alpha_i&lt;c<br>$$</p></li></ul><h3 id="坐标上升法（Coordinate-Ascent）"><a href="#坐标上升法（Coordinate-Ascent）" class="headerlink" title="坐标上升法（Coordinate Ascent）"></a>坐标上升法（Coordinate Ascent）</h3><p>考虑优化问题：<br>$$<br>\max W(\alpha_1, \alpha_2, …, \alpha_m)<br>$$<br>不考虑约束条件，</p><p>重复 {</p><p>​    For i = 1 to m:<br>$$<br>\alpha_i := \arg \max_{\hat{\alpha}_i} W(\alpha_1,\ldots,{\hat{\alpha}}_i,\ldots,\alpha_m)<br>$$<br>} 直到收敛；</p><p>这个算法，可以认为是执行了以下这个过程（以m=2为例）：</p><p><img src="http://s3.sinaimg.cn/middle/b09d46024e1a2f5cf49c2&amp;690" alt="坐标上升法，不断沿着坐标轴方向前进"></p><h3 id="顺序最小优化算法（Sequential-minimal-optimization-SMO）"><a href="#顺序最小优化算法（Sequential-minimal-optimization-SMO）" class="headerlink" title="顺序最小优化算法（Sequential minimal optimization, SMO）"></a>顺序最小优化算法（Sequential minimal optimization, SMO）</h3><p>顺序最小优化算法的基本理念就是在坐标上升法的基础上，改成一次性优化其中两个$\alpha$，而固定其他的$m-2$个$\alpha$。</p><p>假如我们更新$\alpha_1, \alpha_2$：</p><p>因为在之前我们提到:<br>$$<br>\sum_i^m \alpha_iy^{(i)}=0<br>$$<br>于是有：<br>$$<br>\alpha_1 y^{(1)}+\alpha_2 y^{(2)} = -\sum_{i=3}^m\alpha_iy^{(i)}= \zeta<br>$$<br>那么<br>$$<br>\alpha_1=\frac{\zeta-\alpha_2y^{(2)}}{y^{(1)}}<br>$$</p><p>$$<br>W(\alpha_1, \alpha_2, …, \alpha_m) = W(\frac{\zeta-\alpha_2y^{(2)}}{y^{(1)}}, \alpha_2, …, \alpha_m)<br>$$</p><p>在非线性决策边界优化问题中，其实$W$是一个关于$\alpha_i$的二次函数，固定其他$\alpha$之后，$W$函数就可以被简化为：<br>$$<br>a\alpha_2^2+b \alpha_2 + c<br>$$<br>很容易就能求解。</p>]]></content>
      
      <categories>
          
          <category> SVM </category>
          
      </categories>
      
      
        <tags>
            
            <tag> SVM </tag>
            
            <tag> 非线性决策边界 </tag>
            
            <tag> 坐标上升法 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>12. SVM（三）核函数</title>
      <link href="/blog-cs229/2018/07/22/12.%20SVM%EF%BC%88%E4%B8%89%EF%BC%89%E6%A0%B8%E5%87%BD%E6%95%B0/"/>
      <url>/blog-cs229/2018/07/22/12.%20SVM%EF%BC%88%E4%B8%89%EF%BC%89%E6%A0%B8%E5%87%BD%E6%95%B0/</url>
      <content type="html"><![CDATA[<p>在SVM(二)中，我们看到了如下的表示形式：<br>$$<br>W(\alpha)=\sum_{i=1}\alpha_i-\frac{1}{2}\sum_{i=1}\sum_{j=1}\alpha_i\alpha_jy_iy_j(x_i \cdot x_j)<br>$$<br>这里，内积$(x_i \cdot x_j)$就是最简单的核函数的形式。一般核函数会被写成$\langle x^{(i)}, x^{(j)} \rangle$的形式。</p><p>有时候，我们会将一些特征转换到高维空间上，就像我们在之前的<a href="https://jackieanxis.github.io/blog-cs229/2018/06/04/3.%20%E8%BF%87%E6%8B%9F%E5%90%88&amp;%E5%B1%80%E9%83%A8%E5%8A%A0%E6%9D%83%E5%9B%9E%E5%BD%92/">3. 过拟合&amp;局部加权回归</a>中提到的，比如特征$x$表示的是房屋面积，我们需要预测房子是否会在6个月内被卖出，我们有时候会将这个特征映射成如下的形式：<br>$$<br>x \rightarrow \begin{bmatrix}<br>x \\<br>x^2 \\<br>x^3 \\<br>x^4<br>\end{bmatrix} = \phi(x)<br>$$<br>原先的特征的内积形式$\langle x^{(i)}, x^{(j)} \rangle$会被写成$\langle \phi(x^{(i)}), \phi(x^{(j)}) \rangle$，而且往往$\phi(x)$会有很高的维度。因为在很多情况下，计算$\phi(x)$会有很高的代价，或者表示$\phi(x)$需要很高的代价，但是光是计算内核则可能代价较小。</p><p>比如：假如有两个输入：$x, z \in \Bbb R^n$，核函数被定义为：<br>$$<br>\begin{align}<br>k(x, z) = (x^T z)^2 &amp;= (\sum_{i=1}^nx_iz_i)(\sum_{j=1}^nx_jz_j) \\<br>&amp;=\sum_{i=1}^n\sum_{j=1}^n(x_ix_j)(z_iz_j) \\<br>&amp;= \phi(x)^T\phi(z)<br>\end{align}<br>$$<br>假如需要表示成高维向量，那么$\phi(x)$是一个$n \times n$维的向量，如果$n = 3$：<br>$$<br>\phi(x) = \begin{bmatrix}<br>x_1x_1 \\<br>x_1x_2 \\<br>x_1x_3 \\<br>x_2x_1 \\<br>\vdots \\<br>x_3x_3<br>\end{bmatrix}<br>$$<br>所以，计算$\phi(x)$的时间复杂度就达到了$O(n^2)$，而计算核函数仅仅需要计算$x^Tz$，复杂度为$O(n)$。</p><p>接下去我们为这个核函数增加常数项：<br>$$<br>k(x,z)=(x^Tz+c)^2<br>$$<br>那么：<br>$$<br>\phi(x) = \begin{bmatrix}<br>x_1x_1 \\<br>x_1x_2 \\<br>x_1x_3 \\<br>x_2x_1 \\<br>\vdots \\<br>x_3x_3 \\<br>\sqrt{2c}x_1 \\<br>\sqrt{2c}x_2 \\<br>\sqrt{2c}x_3 \\<br>c<br>\end{bmatrix}<br>$$<br>更一般的：<br>$$<br>k(x, z)=(x^Tz+c)^d<br>$$</p><p>有了核函数，即可替换SVM中的内积$\langle x^{(i)}, x^{(j)} \rangle$，比如常用的高斯核：<br>$$<br>k(x,z)=\exp(-\frac{||x-z||^2}{2\sigma^2})<br>$$<br>有了核函数，相当于把数据从原始空间转换到了高位空间，很多数据，在一维空间往往是线性不可分的，但是到了高维空间会变成可分的：</p><p><img src="http://jackie-image.oss-cn-hangzhou.aliyuncs.com/18-7-23/7489488.jpg" alt=""></p><h2 id="核函数的合法性"><a href="#核函数的合法性" class="headerlink" title="核函数的合法性"></a>核函数的合法性</h2><p>如何判断一个核函数是合法的呢？判断依据是：是否存在函数$\phi$，使得$k(x,z)$能够被写成$\langle \phi(x), \phi(z) \rangle$。</p><blockquote><p><strong>定理</strong>：如果核函数合法，那么其对应的核矩阵（kernel matrix）是半正定的。</p></blockquote><p>核矩阵指的是矩阵$K \in \Bbb R^{m\times m}$，其中$K_{ij}=k(x^{(i)}, x^{(j)})$。半正定的意思是，对于任意向量$z$，都存在$z^TKz \geq 0$，证明如下：<br>$$<br>\begin{align}<br>z^TKz &amp;= \sum_i\sum_jz_iK_{ij}z_j \\<br>&amp;= \sum_i\sum_jz_i\phi_(x^{(i)})^T\phi_(x^{(j)})z_j \\<br>&amp;= \sum_i\sum_jz_i\cdot \sum_k\phi_(x^{(i)})_k\underbrace{\phi_(x^{(j)})_k}_{向量第k项} \cdot z_j \\<br>&amp;= \sum_k\sum_i\sum_jz_i\cdot \phi_(x^{(i)})_k\phi_(x^{(j)})_k \cdot z_j \\<br>&amp;= \sum_k(\sum_iz_i\phi(x^{(i)}))^2 \geq 0<br>\end{align}<br>$$<br>事实上，上面的定理的逆命题也一样成立，总结起来：</p><blockquote><p><strong>Merce定理</strong>：给定核函数$k(x, z)$，那么$k(x, z)$合法（也即$\exists \phi, k(x,z)=\phi(x)^T\phi(z)$），当且仅当，对所有的$\lbrace x^{(1)}, \ldots, x^{(m)} \rbrace$，核矩阵$K \in \Bbb R^{m\times m}$是一个对称的半正定矩阵。</p></blockquote>]]></content>
      
      <categories>
          
          <category> SVM </category>
          
      </categories>
      
      
        <tags>
            
            <tag> SVM </tag>
            
            <tag> 核函数 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>11. SVM（二）最优间隔分类器</title>
      <link href="/blog-cs229/2018/07/03/11.%20SVM%EF%BC%88%E4%BA%8C%EF%BC%89%E6%9C%80%E4%BC%98%E9%97%B4%E9%9A%94%E5%88%86%E7%B1%BB%E5%99%A8/"/>
      <url>/blog-cs229/2018/07/03/11.%20SVM%EF%BC%88%E4%BA%8C%EF%BC%89%E6%9C%80%E4%BC%98%E9%97%B4%E9%9A%94%E5%88%86%E7%B1%BB%E5%99%A8/</url>
      <content type="html"><![CDATA[<p><strong>最优间隔分类器</strong>（<em>Optimal Margin Classifier</em>）。其目标是使得最小几何间隔最大化（<a href="https://jackieanxis.github.io/blog-cs229/2018/07/02/10.%20SVM%EF%BC%88%E4%B8%80%EF%BC%89%E6%A6%82%E5%BF%B5/">10. SVM（一）概念</a>）：<br>$$<br>\text{目标(1):} \\<br>\max_{w, b} \gamma \\<br>\text{ s.t. } y^{(i)} \cdot ((\frac{w}{||w||})^T \cdot x^{(i)}+\frac{b}{||w||}) \geq \gamma, i=1,\ldots,n<br>$$<br>我们知道，$\hat{\gamma} = \frac{\gamma}{||w||}$，所以上面的目标可以等同于：<br>$$<br>\text{目标(2):} \\<br>\max_{w, b} \frac{\hat{\gamma}}{||w||} \\<br>\text{ s.t. }y^{(i)} \cdot (w^T \cdot x^{(i)}+b) \geq \hat{\gamma}, i=1,\ldots,n<br>$$<br>为了最大化上述值，我们有两种策略。</p><ol><li>增大$\hat{\gamma}$</li><li>减小$||w||$</li></ol><p>针对第一种可能，我们要证明其无效性。假如，我们增大$\hat{\gamma}$到${\hat{\gamma}}_1 := \lambda {\hat{\gamma}}$，因为$\hat{\gamma}=y(w^Tx+b)$，可以视作$w_1:=\lambda w, b_1 = \lambda b$。所以，此时<br>$$<br>\frac{\hat{\gamma_1}}{||w_1||}=\frac{\lambda \hat{\gamma}}{||\lambda w||} = \frac{\hat{\gamma}}{||w||} \\<br>$$<br>没有发生任何改变，所以第一条策略不可行。于是，我们可以固定$\hat{\gamma}=1$</p><p>此时，上述目标(2)可以表述成：<br>$$<br>\text{目标(3):} \\<br>\min_{w, b} \frac{1}{2}||w||^2 \\<br>\text{ s.t. }y^{(i)} \cdot (w^T \cdot x^{(i)}+b) \geq 1, i=1,\ldots,n<br>$$</p><p>因为最小化$||w||$和最小化$\frac{1}{2}||w||^2$是一致的。</p><h2 id="拉格朗日乘子法（Lagrange-Multiplier）"><a href="#拉格朗日乘子法（Lagrange-Multiplier）" class="headerlink" title="拉格朗日乘子法（Lagrange Multiplier）"></a>拉格朗日乘子法（Lagrange Multiplier）</h2><p>为了解决上述的<strong>凸优化问题</strong>，我们引入拉格朗日乘子法<em>Lagrange Multiplier</em>来解决这个问题。</p><p>我们首先来看看<strong>凸优化问题</strong>的定义：<br>$$<br>\min_wf(w) \\<br>\text{s.t. }g_i(w) \leq 0, h_i(w) =0<br>$$<br>构建拉格朗日乘子：<br>$$<br>{\cal L}(w, \alpha, \beta) = f(w)+\sum_i\alpha_ig_i(w)+\sum_i\beta_ih_i(w)<br>$$<br>定义：<br>$$<br>\theta_p(w) = \max_{\alpha_i&gt;0, \beta}{\cal L}(w, \alpha, \beta)<br>$$<br>观察$\theta_p(w)$：</p><ol><li>如果$g_i(w)&gt;0$，那么$\theta_p(w)=+\infty$（因为$\alpha$可以取任意大值）。</li><li>如果$h_i(w) \neq 0$，那么$\theta_p(w)=+\infty$（因为$\beta$可以取$+\infty/-\infty$）。</li></ol><p>所以，在满足约束的情况下，$\theta_p(w)=f(w)$，$\min_w \theta_p(w)=\min_w f(w)$，因为使得${\cal L}(w, \alpha, \beta)$最大的方法，就是其他所有项全是0。那么，可以得出这样的结论：<br>$$<br>\theta_p(w)=\begin{cases}<br>f(w), &amp;\text{满足约束} \\<br>\infty, &amp;\text{不满足约束}<br>\end{cases}<br>$$<br>因此，在满足条件的情况下，$\min_w\theta_p(w)$等价于$min_wf(w)$。</p><p>我们将最优间隔分类器的目标重新表示一下：<br>$$<br>p^\ast =\min_{w, b}\max_\alpha {\cal L(w, \alpha, b)} \\<br>{\cal L}(w, \alpha, b) = \frac{1}{2}||w||^2+\sum_i\alpha_i(1-y^{(i)}(w^T x^{(i)}+b))<br>$$<br>其中，直接忽略了$h_i(w)=0$的约束，而$g_i(w,b)=1-y^{(i)}(w^Tx^{(i)}+b) \leq 0, f(w)=\frac{1}{2}||w||^2$</p><h2 id="对偶问题（Dual-Problem）"><a href="#对偶问题（Dual-Problem）" class="headerlink" title="对偶问题（Dual Problem）"></a>对偶问题（Dual Problem）</h2><p>一般来说，将原始问题转化成对偶问题来求解。一是因为对偶问题往往比较容易求解，二是因为对偶问题引入了核函数，方便推广到非线性分类的情况。</p><p>我们看到，之前的原始问题，是<br>$$<br>p^\ast =\min_{w, b}\max_\alpha {\cal L}(w, \alpha, b)<br>$$</p><p>那么，定义其对偶问题：</p><p>$$<br>l^\ast =\max_\alpha\min_{w,b}{\cal L}(w, \alpha, b)<br>$$</p><p>接下去，我们求解对偶问题：</p><p>先求解$\min_{w,b}{\cal L}(w, \alpha, b)$：</p><p>分别求偏导，使其等于0，导出最小值：<br>$$<br>\begin{align}<br>&amp; \nabla_w{\cal L}(w, \alpha, b) =w-\sum_{i=1}\alpha_iy^{(i)}x^{(i)}=0 \\<br>&amp; \nabla_b{\cal L}(w, \alpha, b) =\sum_{i=1}\alpha_iy^{(i)}=0<br>\end{align}<br>$$</p><p>得到：</p><p>$$<br>w =\sum_{i=1}\alpha_iy^{(i)}x^{(i)} \\<br>\sum_{i=1}\alpha_iy^{(i)} = 0<br>$$</p><p>代入${\cal L}(w, \alpha, b)$，就可以得到最小值：</p><p>$$<br>\begin{align}<br>{\cal L}(w, \alpha, b) &amp;= \frac{1}{2}||w||^2+\sum_i\alpha_i(1-y^{(i)}(w^T x^{(i)}+b)) \\<br>\min_{w, b}{\cal L}(w, \alpha, b) &amp;=\underbrace{\sum_{i=1}\alpha_i-\frac{1}{2}\sum_{i=1}\sum_{j=1}\alpha_i\alpha_jy_iy_j(x_i \cdot x_j)}_{W(\alpha)}<br>\end{align}<br>$$</p><p>于是，我们的对偶问题简化到了对$W(\alpha)$最大化：<br>$$<br>\max_\alpha W(\alpha) \\<br>\text{s.t. }\alpha_i \geq 0, \sum_iy_i\alpha_i=0<br>$$</p><p>假设，我们解得的对偶问题的解为：$\alpha^\ast =[\alpha_1^\ast ,\alpha_2^\ast , \ldots, \alpha_m^\ast ]$，那么最终原始问题的解可以表示成：</p><p>$$<br>w^\ast =\sum_{i=1}\alpha_i^\ast y^{(i)}x^{(i)}<br>$$</p><p>在原始问题中，还有$b$未得到解决。我们先来观察一下约束项：<br>$$<br>g_i(w,b)=1-y{(i)}(w^Tx^{(i)}+b) \leq 0<br>$$<br><img src="http://jackie-image.oss-cn-hangzhou.aliyuncs.com/18-7-4/9336603.jpg" alt=""></p><p>我们知道，在数据中，只有少数的几个数据点，他们的函数距离为1（最小），也即$g_i(w,b)=0$，如图所示。</p><p>在整个数据集中，只有这些数据点对约束超平面起了作用，这些数据点被称为支持向量（<em>support vector</em>），其对应的$\alpha_i^\ast  \neq 0$，而其他不是支持向量的数据点，没有对约束超平面起作用，其$\alpha_i^\ast =0$。</p><p>此时，我们已经得到了$w^\ast $，而$b^\ast $的计算如下，找到一个数据点，其$\alpha_j^\ast  \neq 0$(也就是支持向量，其函数间隔为1)，我们就能得到：</p><p>$$<br>y^{(j)}(w^{*T}x^{(j)}+b^\ast )=1<br>\Rightarrow<br>b^\ast =y^{(j)}-\sum_{i=1}\alpha_i^\ast y^{(i)}(x^{(i)} \cdot x^{(j)})<br>$$</p>]]></content>
      
      <categories>
          
          <category> SVM </category>
          
      </categories>
      
      
        <tags>
            
            <tag> SVM </tag>
            
            <tag> 最优间隔分类器 </tag>
            
            <tag> 拉格朗日乘子法 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>10. SVM（一）概念</title>
      <link href="/blog-cs229/2018/07/02/10.%20SVM%EF%BC%88%E4%B8%80%EF%BC%89%E6%A6%82%E5%BF%B5/"/>
      <url>/blog-cs229/2018/07/02/10.%20SVM%EF%BC%88%E4%B8%80%EF%BC%89%E6%A6%82%E5%BF%B5/</url>
      <content type="html"><![CDATA[<p><em>SVM</em>，指的是支持向量机（<em>support vector machines</em>）。</p><p>支持向量机，假设数据是线性可分的，那么我们就能找到一个超平面，将数据分成两类。但是一旦线性可分，我们就可能找到无数的超平面，都可以将数据分成两类：</p><p><img src="http://jackie-image.oss-cn-hangzhou.aliyuncs.com/18-7-3/83131877.jpg" alt=""></p><p>但是很明显，上图中虽然<em>a, c</em>都对数据进行了有效的分割。但很明显，都不如<em>b</em>分割的好。</p><p>我们可以用“间隔”这个概念来定义这个超平面（在二维上是线）对数据的分割优劣。在分类正确的情况下，间隔越大，我们认为对数据的分类越好。</p><p>我们的目标是得到数据的分类：$y \in \lbrace -1, +1 \rbrace$。</p><p>这个超平面，则可以表示成$w^Tx+b$，其中$w=[\theta_1, \ldots, \theta_n]^T, b=\theta_0$。这个超平面可以表达成一个$n+1$维向量。</p><p>判别函数：<br>$$<br>g(z)=\begin{cases}<br>+1, &amp; \text{如果$z\geq0$} \\<br>-1, &amp; \text{otherwise}<br>\end{cases}<br>$$<br>假设则可以表示成：$h_{w,b}(x)=g(w^Tx+b)$</p><h2 id="间隔"><a href="#间隔" class="headerlink" title="间隔"></a>间隔</h2><h3 id="函数间隔（functional-margin）"><a href="#函数间隔（functional-margin）" class="headerlink" title="函数间隔（functional margin）"></a>函数间隔（functional margin）</h3><p>某个超平面$(w,b)$和训练样本$(x^{(i)}, y^{(i)})$之间的函数间隔被表示成：<br>$$<br>\hat{\gamma}^{(i)}=y^{(i)}(w^Tx^{(i)}+b)<br>$$<br>于是，我们可以知道：</p><ol><li>当$y^{(i)}=1$，于是我们想获得更大的函数间隔（这是我们的目标），就需要使得$w^Tx^{(i)}+b \gg 0$</li><li>相反，当$y^{(i)}=-1$，我们想获得更大的函数间隔，就需要使得$w^Tx^{(i)}+b \ll 0$</li></ol><p>并且，很明显，只有当函数间隔$\hat{\gamma}&gt;0$时，分类结果是正确的。</p><p>最后，超平面与数据集$\lbrace (x^{(1)}, y^{(1)}), (x^{(2)}, y^{(2)}), \ldots \rbrace$之间的函数间隔，被定义为所有函数间隔中的最小值：<br>$$<br>\hat{\gamma}=\min_i\hat{\gamma}^{(i)}<br>$$</p><h3 id="几何间隔（geometric-margin）"><a href="#几何间隔（geometric-margin）" class="headerlink" title="几何间隔（geometric margin）"></a>几何间隔（geometric margin）</h3><p><img src="http://jackie-image.oss-cn-hangzhou.aliyuncs.com/18-7-3/34659777.jpg" alt=""></p><p>从点$(x^{(i)}, y^{(i)})$出发，对超平面做垂线，得到点D，我们知道他们之间的距离，就是该超平面到数据点$(x^{(i)}, y^{(i)})$的几何间隔。</p><p>经过推导，D的坐标可以表示为：<br>$$<br>x^{(i)}-\gamma^{(i)}\frac{w}{||w||}<br>$$<br>又因为，D在超平面$w^Tx+b=0$上，所以：<br>$$<br>\begin{align}<br>&amp; w^T(x^{(i)}-\gamma^{(i)}\frac{w}{||w||})+b=0 \\<br>&amp; \Rightarrow w^Tx^{(i)}+b=\gamma^{(i)} \cdot \frac{w^Tw}{||w||}=\gamma^{(i)} \cdot ||w|| \\<br>&amp; \Rightarrow \gamma^{(i)}=(\frac{w}{||w||})^T \cdot x^{(i)}+\frac{b}{||w||}<br>\end{align}<br>$$<br>加上正负分类的判断：<br>$$<br>\gamma^{(i)}=y^{(i)} \cdot ((\frac{w}{||w||})^T \cdot x^{(i)}+\frac{b}{||w||})<br>$$<br>我们可以看到，几何间隔跟函数间隔之间存在如下的关系：<br>$$<br>\hat{\gamma}^{(i)} = \frac{\gamma^{(i)}}{||w||}<br>$$</p><p>同样的，超平面与数据集$\lbrace (x^{(1)}, y^{(1)}), (x^{(2)}, y^{(2)}), \ldots \rbrace$之间的几何间隔，被定义为所有几何间隔中的最小值：<br>$$<br>\gamma=\min_i\gamma^{(i)}<br>$$<br>最后，我们导出<strong>最优间隔分类器</strong>（<em>Optimal Margin Classifier</em>）问题：选择$w, b$，最大化$\gamma$，同时满足$\forall(x^{(i)}, y^{(i)})$，$ y^{(i)} \cdot ((\frac{w}{||w||})^T \cdot x^{(i)}+\frac{b}{||w||}) \geq \gamma$（所有数据点的几何间隔都大于该最小几何间隔）。</p><p>目前为止，已经是SVM问题的一个简化版本。</p>]]></content>
      
      <categories>
          
          <category> SVM </category>
          
      </categories>
      
      
        <tags>
            
            <tag> SVM </tag>
            
            <tag> 函数间隔 </tag>
            
            <tag> 几何间隔 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>9. 生成学习算法的例子</title>
      <link href="/blog-cs229/2018/06/29/9.%20%E7%94%9F%E6%88%90%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%9A%84%E4%BE%8B%E5%AD%90/"/>
      <url>/blog-cs229/2018/06/29/9.%20%E7%94%9F%E6%88%90%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%9A%84%E4%BE%8B%E5%AD%90/</url>
      <content type="html"><![CDATA[<h2 id="例一：高斯判别分析和logistic函数"><a href="#例一：高斯判别分析和logistic函数" class="headerlink" title="例一：高斯判别分析和logistic函数"></a>例一：高斯判别分析和logistic函数</h2><p>我们来看一个例子，对于一个高斯判别分析问题，根据贝叶斯：<br>$$<br>\begin{align}<br>p(y=1|x) &amp;= \frac{p(x|y=1)p(y=1)}{p(x)} \\<br>&amp;= \frac{p(x|y=1)p(y=1)}{p(x|y=0)p(y=0)+p(x|y=1)p(y=1)}<br>\end{align}<br>$$<br>在这里，我们提出几个假设：</p><ol><li>$p(y)$是均匀分布的，也就是$p(y=1)=p(y=0)$</li><li>$x$的条件概率分布（$p(x|y=0)$和$p(x|y=1)$）满足高斯分布。</li></ol><p>考虑二维的情况：</p><p><img src="http://jackie-image.oss-cn-hangzhou.aliyuncs.com/2018-06-30-084350.png" alt="image-20180630164349595"></p><p>蓝色数据表达的是$p(x|y=0)$的分布，红色数据表达的是$p(x|y=1)$的分布，两条蓝色和红色的曲线分别是它们的概率密度曲线。</p><p>而灰色的曲线则表示了$p(y=1|x)$的概率密度曲线。</p><p>假设$p(x|y=0) \sim N(\mu_0, \sigma_0)$，$p(x|y=1) \sim N(\mu_1, \sigma_1)$，而$p(y)$均匀分布那么：<br>$$<br>\begin{align}<br>p(y=1|x) &amp;= \frac{N(\mu_0,\sigma_0)}{N(\mu_0,\sigma_0)+N(\mu_1,\sigma_1)} \\<br>&amp;= \cdots \\<br>&amp;= \frac{1}{1+\frac{\sigma_0}{\sigma_1}exp(2\sigma_1^2(x-\mu_0)^2-2\sigma_0^2(x-\mu_1)^2}<br>\end{align}<br>$$<br>事实上，这条曲线跟我们之前见过的<em>logistic</em>曲线非常像，特别是当我们假设$\sigma_0=\sigma_1$的时候，就是一条<em>logistic</em>曲线。</p><p>我们有如下的推广结论：<br>$$<br>{\begin{cases}<br>p(x|y=1) \sim Exp Family(\eta_1) \\<br>p(x|y=0) \sim Exp Family(\eta_0)<br>\end{cases}} \Rightarrow p(y=1|x)是logistic函数<br>$$<br>但这个命题的逆命题并不成立，故而我们知道，<em>logistic</em>所需要的假设更少（无需假设$x$的条件概率分布），鲁棒性更强。而生成函数因为对数据的分布做出了假设，所以需要的数据量会少于<em>logstic</em>回归，我们需要在两者之间进行权衡。</p><h2 id="例二：垃圾邮件分类（1）"><a href="#例二：垃圾邮件分类（1）" class="headerlink" title="例二：垃圾邮件分类（1）"></a>例二：垃圾邮件分类（1）</h2><p>这里我们会用朴素贝叶斯（Naive Bayes）来解决垃圾邮件分类问题（$y\in \lbrace 0, 1 \rbrace$）。</p><p>首先对邮件进行建模，生成特征向量如下：<br>$$<br>x=<br>\begin{bmatrix}<br>0 \\<br>0 \\<br>0 \\<br>\vdots \\<br>1 \\<br>\vdots<br>\end{bmatrix}<br>\begin{matrix}<br>a \\<br>advark \\<br>ausworth \\<br>\vdots \\<br>buy \\<br>\vdots<br>\end{matrix}<br>$$<br>这是一个类似于词频向量的特征向量，我们有一个50000个词的词典，如果邮件中出现了某个词汇，那么其在向量中对应的位置就会被标记为1，否则为0。</p><p>我们的目标是获取，垃圾邮件和非垃圾邮件的特征分别是怎么样的，也即$p(x|y)$。$x={\lbrace 0, 1 \rbrace}^n, y \in \lbrace 0, 1 \rbrace$，这里我们的词典中词汇数量是50000，所以$n=50000$，特征向量$x$会有$2^{50000}$种可能，需要$2^{50000}-1$个参数。</p><p>我们假设$x_i|y$之间相互独立(虽然假设各个单词的出现概率相互独立不是很合理，但是即便这样，朴素贝叶斯的效果依旧不错)，根据朴素贝叶斯，我们得到：<br>$$<br>p(x_1, x_2, \ldots, x_{50000}|y)=p(x_1|y)p(x_2|y) \cdots p(x_{50000}|y)<br>$$<br>单独观察$p(x_j|y=1)​$：<br>$$<br>p(x_j|y=1) = p(x_j=1|y=1)^{x_j}p(x_j=0|y=1)^{1-x_j}<br>$$<br>给定三个参数：<br>$$<br>\begin{align}<br>\phi_{j|y=1} &amp;= p(x_j=1|y=1) \\<br>\phi_{j|y=0} &amp;= p(x_j=1|y=0) \\<br>\phi_y &amp;= p(y = 1)<br>\end{align}<br>$$<br>故：<br>$$<br>\begin{align}<br>p(x_j|y=1) &amp;= \phi_{j|y=1}^{x_j}(\phi_y - \phi_{j|y=1})^{1-x_j}\\<br>p(x_j|y=0) &amp;= \phi_{j|y=0}^{x_j}(1-\phi_y + \phi_{j|y=0})^{1-x_j} \\<br>p(x_j|y) &amp;= p(x_j|y=1)^yp(x_j|y=0)^{1-y} \\<br>p(y) &amp;= \phi_y^y(1-\phi_y)^{1-y}<br>\end{align}<br>$$<br>按照上个博客<a href="https://jackieanxis.github.io/blog-cs229/2018/06/29/8.%20%E7%94%9F%E6%88%90%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%9A%84%E6%A6%82%E5%BF%B5/">8. 生成学习算法的概念</a>中所述，我们会选用联合概率分布的极大似然来导出最优解：<br>$$<br>l(\phi_y,\phi_{j|y=1},\phi_{j|y=0}=\prod_{i=1}^mp(x^{(i)},y^{(i)})=\prod_{i=1}^mp(x^{(i)}|y^{(i)})p(y^{(i)})<br>$$</p><p>可以解得：<br>$$<br>\begin{align}<br>\phi_{j|y=1} &amp;= \frac{\sum_{i=1}^m1\lbrace x_j{(i)}=1, y^{(i)}=1 \rbrace}{\sum_{i=1}^m1\lbrace y^{(i)}=1 \rbrace}  = \frac{统计所有包含词语j的垃圾邮件的数量}{垃圾邮件的总数}\\<br>\phi_{j|y=0} &amp;= \frac{\sum_{i=1}^m1\lbrace x_j{(i)}=1, y^{(i)}=0 \rbrace}{\sum_{i=1}^m1\lbrace y^{(i)}=0 \rbrace} = \frac{统计所有包含词语j的非垃圾邮件的数量}{非垃圾邮件的总数} \\<br>\phi_y &amp;= \frac{\sum_{i=1}^m1\lbrace y^{(i)}=1 \rbrace}{m} = \frac{垃圾邮件的数量}{邮件的总数}<br>\end{align}<br>$$<br>通过以上的公式，我们已经可以完全推得$p(x_1, x_2, \ldots, x_{50000}|y)$。</p><h3 id="Laplace平滑"><a href="#Laplace平滑" class="headerlink" title="Laplace平滑"></a>Laplace平滑</h3><p>假设，训练集中，我们重来没有碰到过”<em>NIPS</em>“这个词汇，假设我们词典中包含这个词，位置是30000，也就是说：<br>$$<br>\begin{align}<br>p(x_{30000}=1|y=1) &amp;= 0\\<br>p(x_{30000}=0|y=1) &amp;= 0<br>\end{align} \\<br>\Downarrow \\<br>p(x|y=1) =\prod_{i=1}^{50000}p(x_i|y=1)=0 \\<br>p(x|y=0) =\prod_{i=1}^{50000}p(x_i|y=0)=0<br>$$<br>故而在分类垃圾邮件时：<br>$$<br>\begin{align}<br>p(y=1|x) &amp;= \frac{p(x|y=1)p(y=1)}{p(x)} \\<br>&amp;=\frac{p(x|y=1)p(y=1)}{p(x|y=0)p(y=0)+p(x|y=1)p(y=1)} \\<br>&amp;= \frac{0}{0+0}<br>\end{align}<br>$$<br>所以，我们提出$p(x_{30000}=1|y=1) = 0$这样的假设不够好。</p><p><em>Laplace</em>平滑就是来帮助解决这个问题的。</p><p>举例而言，在计算：<br>$$<br>\phi_y=p(y=1)=\frac{\text{numof(1)}}{\text{numof(0)}+\text{numof(1)}}<br>$$<br>其中，$\text{numof(1)}$表示的是，被分类为1的训练集中数据个数。</p><p>在<em>Laplace</em>平滑中，我们会采取如下策略:<br>$$<br>\phi_y=p(y=1)=\frac{\text{numof(1)}+1}{\text{numof(0)}+1+\text{numof(1)}+1}<br>$$<br>比如，A球队在之前的五场比赛里面都输了，我们预测下一场比赛赢的概率：<br>$$<br>p(y=1)=\frac{0+1}{0+1+5+1}=\frac{1}{7}<br>$$<br>而不是简单的认为（没有<em>Laplace</em>平滑）是0。</p><p>推广而言，在多分类问题中，$y\in\lbrace1, \ldots, k \rbrace$，那么：<br>$$<br>p(y=j) = \frac{\sum_{i=1}^m1\lbrace y^{(i)} = j \rbrace+1}{m+k}<br>$$</p><h2 id="例三：垃圾邮件分类（2）"><a href="#例三：垃圾邮件分类（2）" class="headerlink" title="例三：垃圾邮件分类（2）"></a>例三：垃圾邮件分类（2）</h2><p>之前的垃圾分类模型里面，我们对邮件提取的特征向量是：<br>$$<br>x=[1,0,0,\ldots,1,\ldots]^T<br>$$<br>这种模型，我们称之为多元伯努利事件模型（Multivariate Bernoulli Event Model）。</p><p>现在，我们换一种特征向量提取方式，将邮件的特征向量表示为：<br>$$<br>x=[x_1,x_2,\ldots,x_j,\ldots]^T<br>$$<br>$x_j$表示词汇$j$在邮件中出现的次数。上述的特征向量也就是词频向量了。这种模型，我们称为多项式事件模型（Multinomial Event Model）。</p><p>对联合概率分布$p(x,y)$进行极大似然估计，得到如下的参数：<br>$$<br>\begin{align}<br>\phi_{k|y=1} &amp;= p(x_j=k|y=1) = \frac{C_{x=k}+1}{C_{y=1}+n} \\<br>\phi_{k|y=0} &amp;=p(x_j=k|y=0) = \frac{C_{x=k}+1}{C_{y=0}+n} \\<br>\phi_{y} &amp;= p(y=1) = \frac{C_{y=1}+1}{C_{y=1}+1+C_{y=0}+1}<br>\end{align}<br>$$<br>其中：</p><p>$n$表示词典中词汇的数量，也就是特征向量的长度；<br>$$<br>C_{x=k}=\sum_{i=1}^m(1\lbrace y^{(i)}=1 \rbrace \sum_{j=1}^{n_i}1 \lbrace x_j^{(i)} = k \rbrace)<br>$$<br>表示在训练集中，所有垃圾邮件中词汇$k$出现的次数（并不是邮件的次数，而是词汇的次数）；<br>$$<br>C_{y=1}=\sum_{i=1}^n(1\lbrace y^{(i)} = 1 \rbrace \cdot n_i)<br>$$<br>表示训练集中垃圾邮件的所有词汇总长；<br>$$<br>C_{y=0}=\sum_{i=1}^n(1\lbrace y^{(i)} = 0 \rbrace \cdot n_i)<br>$$<br>表示训练集中非垃圾邮件的所有词汇总长；</p>]]></content>
      
      <categories>
          
          <category> 生成学习算法 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 生成学习算法 </tag>
            
            <tag> 拉普拉斯平滑 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>8. 生成学习算法的概念</title>
      <link href="/blog-cs229/2018/06/29/8.%20%E7%94%9F%E6%88%90%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%9A%84%E6%A6%82%E5%BF%B5/"/>
      <url>/blog-cs229/2018/06/29/8.%20%E7%94%9F%E6%88%90%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%9A%84%E6%A6%82%E5%BF%B5/</url>
      <content type="html"><![CDATA[<p>生成学习算法，英文为<em>Generative Learning Algorithm</em>。</p><p>我们之前看到的都是判别学习算法（<em>Discriminative Learning Algorithm</em>）。判别学习算法可以分成两种：</p><ol><li>学得$p(y|x)$，比如之前的线性模型</li><li>学得一个假设$h_\theta (x) = \lbrace 0, 1 \rbrace$，比如二分类问题</li></ol><p>上面都是根据特征$x$，来对输出$y$进行建模，也许$y$是一个连续的值，也可能是离散的，比如类别。</p><p>那么，生成学习算法（<em>Generative Learning Algorithm</em>）刚好跟判别学习算法（<em>Discriminative Learning Algorithm</em>）相反，其用输出$y$对$x$进行建模，也就是：<strong>学得$p(x|y)$</strong>。</p><p><strong>详细解释</strong>：</p><p>对于一个二分类或多分类问题，生成学习算法，在给定样本所述的类别的条件下，会对其样本特征建立一个概率模型。举例而言，在给定癌症是良性或者是恶性的条件下，生成模型会对该癌症的特征的概率分布进行建模。</p><p>根据朴素贝叶斯，<br>$$<br>p(y=1|x)=\frac{p(x|y=1) p(y=1)}{p(x)}<br>$$<br>因为给定了$x$，所以$p(x)$可以视作1，也即<br>$$<br>p(y=1|x)=p(x|y=1) p(y=1)<br>$$</p><h2 id="高斯判别算法"><a href="#高斯判别算法" class="headerlink" title="高斯判别算法"></a>高斯判别算法</h2><p><em>Gaussian Discriminant Algorithm</em></p><p>对特征满足高斯分布的二分类问题进行建模。</p><p>假设$x \in \Bbb R^n$，且$p(x|y)$满足高斯分布，因为$x$往往是一个特征向量，故而这里的高斯分布是一个多元高斯分布（<em>multivariate Gaussian）</em>。</p><blockquote><p>多元高斯分布，$z \sim N(\mu, \Sigma)$。<br>$$<br>p(z)=\frac{1}{\sqrt{(2\pi)^n|\Sigma|}}exp(-\frac{1}{2}(z-\mu)^T\Sigma^{-1}(z-\mu))<br>$$<br>$z$是一个$n$维向量，$\mu$则是均值向量，$\Sigma$是协方差矩阵：$E[(z-\mu)(z-\mu)^T]$。</p></blockquote><p>对于一个二分类判别问题，正则响应函数为$\phi$，那么<br>$$<br>p(y)=\phi^y(1-\phi)^{1-y}<br>$$<br>且<br>$$<br>p(x|y=0) \sim N(\mu_0, \Sigma) \\<br>p(x|y=1) \sim N(\mu_1, \Sigma)<br>$$<br>(这里假设了，对于$y=0$和$y=1$，是两个不同均值，但协方差矩阵相同的多元高斯分布，所以这也是该模型的限制之一)</p><p>我们写出对数似然函数：<br>$$<br>\begin{align}<br>l(\phi, \mu_0, \mu_1, \Sigma) &amp;= log\prod_{i=1}^mp(x^{(i)}, (y^{(i)})) \\<br>&amp;= log\prod_{i=1}^mp(x^{(i)}| (y^{(i)}))p(y^{(i)})<br>\end{align}<br>$$</p><p>对比一下之前<a href="https://jackieanxis.github.io/blog-cs229/2018/06/05/5.%20%E4%BA%8C%E5%88%86%E7%B1%BB%E9%97%AE%E9%A2%98/">二分类问题</a>中的对数似然估计函数：<br>$$<br>l(\theta) = \sum_{i=1}^mlog(P(y^{(i)}|x^{(i)};\theta))<br>$$<br>这里，我们使用了联合概率（<em>joint likelihood</em>）: $p(x^{(i)}, (y^{(i)}))$，而在之前的判别模型里，我们用的是条件概率（<em>conditional likelihood</em>）: $P(y^{(i)}|x^{(i)};\theta)$。因为，在判别模型中，特征$x$是给定的，所以用条件概率来进行极大似然估计；而在生成模型中，特征$x$不给定，所以需要用联合概率来进行极大似然估计。</p><p>然后，我们进行极大似然估计，得到：<br>$$<br>\phi=\frac{1}{m}\sum_{i=1}^my^{(i)}=\frac{1}{m}\sum_{i=1}^m 1\lbrace y^{(i)} = 1 \rbrace<br>$$<br>也就是分类标签为1的训练样本的比例。</p><p>再看$\mu_0, \mu_1$：<br>$$<br>\mu_0=\underbrace{\sum_{i=1}^m 1\lbrace y^{(i)} = 0 \rbrace \cdot x^{(i)}}_{(1)} /\underbrace{ \sum_{i=1}^m 1\lbrace y^{(i)} = 0 \rbrace}_{(2)} \\<br>\mu_1=\sum_{i=1}^m 1\lbrace y^{(i)} = 1 \rbrace \cdot x^{(i)} / \sum_{i=1}^m 1\lbrace y^{(i)} = 1 \rbrace<br>$$<br>式(1)表达了分类为0的样本中，特征$x$的和；式(2)表达了分类为0的样本个数；故而$\mu_0$表达了样本中，分类为0的样本的特征的均值。同理得到$\mu_1$。</p><p>我们再次回顾上面的高斯判别算法，事实上，高斯判别算法，假设了特征$x$满足了多元高斯分布，利用极大似然估计，对不同类别的特征$x$的分布进行了建模。</p><p>下节课我们将会探讨其他的关于生成学习算法的案例。</p>]]></content>
      
      <categories>
          
          <category> 生成学习算法 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 生成学习算法 </tag>
            
            <tag> 高斯判别算法 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>7. 广义线性模型</title>
      <link href="/blog-cs229/2018/06/09/7.%20%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/"/>
      <url>/blog-cs229/2018/06/09/7.%20%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/</url>
      <content type="html"><![CDATA[<p>广义线性模型，英文名为<strong>Generalized Linear Model</strong>，简称GLM。</p><p>之前，涉及到两种的两种模型：</p><ol><li>线性拟合模型，假设了$P(y|x;\theta)$是高斯分布</li><li>二分类问题，假设了$P(y|x;\theta)$满足伯努利分布</li></ol><p>但以上两者知识一种更广泛的，被称为『指数分布族』（The Exponential Family）的特例。</p><h2 id="指数分布族"><a href="#指数分布族" class="headerlink" title="指数分布族"></a>指数分布族</h2><p>$$<br>P(y;\eta)=b(y)exp(\eta^TT(y)-a(\eta))<br>$$</p><p>可以被表示为以上形式的分布，都是指数分布族的某个特定分布，给定$a, b, T$，就可以定义一个概率分布的集合，以$\eta$为参数，就可以得到不同的概率分布。</p><p>在广义线性模型中，会假设$\eta=\theta^Tx$，也就是$\eta$和特征$x$线性相关。</p><h2 id="伯努利分布"><a href="#伯努利分布" class="headerlink" title="伯努利分布"></a>伯努利分布</h2><p>首先，我们给出$y=1$的概率：<br>$$<br>P(y=1;\phi)=\phi<br>$$<br>于是：<br>$$<br>\begin{align}<br>P(y;\phi)<br>    &amp;= \phi^y(1-\phi)^T\\<br>    &amp;= exp(log(\phi^T(1-\phi^T)))\\<br>    &amp;= exp(ylog(\phi)+(1-y)log(1-\phi))\\<br>    &amp;= exp(log\frac{\phi}{1-\phi} \cdot y + log(1-\phi))<br>\end{align}<br>$$<br>比较我们上面的概率形式和指数分布族的标准形式，可以得到：<br>$$<br>\begin{cases}<br>\eta &amp;= log\frac{\phi}{1-\phi}, \text{于是} \phi=\frac{1}{1+e^{-\eta}}\\<br>a(\eta) &amp;= -log(1-\phi)=log(1+e^\eta)\\<br>T(y) &amp;= y\\<br>b(y) &amp;= 1<br>\end{cases}<br>$$</p><p>这里的$\phi$一般会被称为正则响应函数（<em>canonic response function</em>）：<br>$$<br>g(\eta) = E[y|\eta]=\frac{1}{1+e^{-\eta}}<br>$$<br>相对的，正则关联函数（<em>canonic link function</em>）则是$g^{-1}$。</p><h2 id="高斯分布"><a href="#高斯分布" class="headerlink" title="高斯分布"></a>高斯分布</h2><p>$$<br>N(\mu,\sigma^2)=\frac{1}{\sqrt{2\pi}}exp(-\frac{1}{2}(y-\mu)^2)<br>$$</p><p>这里，出于简洁考虑，假设$\sigma=1$，经过一系列化简后，可以表示成：<br>$$<br>\frac{1}{\sqrt{2\pi}} \cdot exp(-\frac{1}{2}y^2) \cdot exp(\mu y-\frac{1}{2}\mu^2)<br>$$<br>那么，<br>$$<br>\begin{cases}<br>\eta &amp;= \mu\\<br>a(\eta) &amp;= \frac{1}{2}\mu^2=\frac{1}{2}\eta^2\\<br>T(y) &amp;= y\\<br>b(y) &amp;= \frac{1}{\sqrt{2\pi}} \cdot exp(-\frac{1}{2}y^2)<br>\end{cases}<br>$$</p><h2 id="多项式分布"><a href="#多项式分布" class="headerlink" title="多项式分布"></a>多项式分布</h2><h4 id="建模"><a href="#建模" class="headerlink" title="建模"></a>建模</h4><p>在二项分布中，$y\in \lbrace 1, 2 \rbrace$</p><p>而多项式分布，$y \in \lbrace 1,\cdots, k \rbrace$</p><p>一般会被用来进行邮件分类或者进行病情分类等等</p><p>我们假设<br>$$<br>P(y=i)=\phi_i<br>$$<br>也即，邮件属于$i$类的概率是$\phi_i$，是关于特征$x$的一个函数。</p><p>那么，可以用$k$个参数来建模多项式分布<br>$$<br>P(y)=\prod_{i=1}^k\phi_i^{1\lbrace y=i \rbrace}<br>$$</p><p>其中，$1 \lbrace \cdots \rbrace$的含义为，检验$\cdots$是否为真命题，若为真命题，则取1，否则取0。</p><p>因为所有概率和为1，所以最后一个参数<br>$$<br>\begin{align}<br>\phi_k &amp;= 1-\sum_{j=1}^{k-1}\phi_j \\<br>1 \lbrace y=k \rbrace &amp;=1-\sum_{j=1}^{k-1}1 \lbrace y=j \rbrace<br>\end{align}<br>$$<br>经过化简，也可以表示成：<br>$$<br>P(y)=exp[\sum_{i=1}^{k-1}(log(\frac{\phi_i}{\phi_k}) \cdot 1\lbrace y=i \rbrace )] + log(\phi_k)<br>$$<br>故而<br>$$<br>\eta = \begin{bmatrix}<br>log(\frac{\phi_1}{\phi_k}) \\<br>\vdots \\<br>log(\frac{\phi_{k-1}}{\phi_k})<br>\end{bmatrix} \in \Bbb R^{k-1}<br>$$<br>$$<br>a(\eta) = -log(\phi_k)<br>$$<br>$$<br>T(y)= \begin{bmatrix}<br>1 \lbrace y=1 \rbrace \\<br>\vdots \\<br>1 \lbrace y=k-1 \rbrace<br>\end{bmatrix} \in (0, 1)^{k-1}<br>$$<br>$$<br>b(y) = 1<br>$$</p><p>根据$\eta$可得：<br>$$<br>\phi_i = e^{\eta_i} \cdot \phi_k<br>$$<br>又因为：<br>$$<br>\sum_{i=1}^{k}\phi_i=\sum_{i=1}^k\phi_ke^{\eta_i}=1<br>$$<br>故而：<br>$$<br>\phi_k = \frac{1}{\sum_{i=1}^ke^{\eta_i}}=\frac{1}{e^{\eta_k}+\sum_{i=1}^{k-1}e^{\eta_i}} = \frac{1}{1+\sum_{i=1}^{k-1}e^{\eta_i}}<br>$$<br>所以：<br>$$<br>\begin{align}<br>\phi_i &amp;= e^{\eta_i} \cdot \phi_k \\<br>&amp;= \frac{e^{\eta_i}}{1 + \sum_{j=1}^{k-1}e^{\eta_j}} \\<br>&amp;= \frac{e^{\theta_i^Tx_i}}{1 + \sum_{j=1}^{k-1}e^{\theta_j^Tx_j}}<br>\end{align}<br>$$<br>上述函数，被称为『softmax』函数，这个函数的作用经常用于进行归一化。</p><p>经过上述步骤，假设函数可以被写成如下形式：<br>$$<br>h_\theta(x)=<br>\left[<br>\begin{array}{c}<br>1\lbrace y=1 \rbrace  \\<br>\vdots \\<br>1\lbrace y=k-1 \rbrace<br>\end{array} | x;\theta<br>\right]=<br>\begin{bmatrix}<br>\phi_1\\<br>\vdots\\<br>\phi_{k-1}<br>\end{bmatrix}<br>$$</p><h4 id="回归"><a href="#回归" class="headerlink" title="回归"></a>回归</h4><p>在经过上述推导，当我们有一堆训练集（$(x^{(1)}, y^{(1)}), \cdots, (x^{(m)}, y^{(m)})$）用于训练的时候，则可以进行极大似然估计：<br>$$<br>L(\theta) = \prod_{i=1}^mP(y^{(i)} | x^{(i)};\theta) = \prod_{i=1}^m\prod_{j=1}^k\phi_j^{1\lbrace y^{(i)}=j \rbrace }<br>$$</p>]]></content>
      
      <categories>
          
          <category> 广义线性模型 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> softmax </tag>
            
            <tag> 指数分布族 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>6. 牛顿法</title>
      <link href="/blog-cs229/2018/06/07/6.%20%E7%89%9B%E9%A1%BF%E6%B3%95/"/>
      <url>/blog-cs229/2018/06/07/6.%20%E7%89%9B%E9%A1%BF%E6%B3%95/</url>
      <content type="html"><![CDATA[<blockquote><p><strong>牛顿法</strong>（英语：Newton’s method）又称为<strong>牛顿-拉弗森方法</strong>（英语：Newton-Raphson method），它是一种在实数域和复数域上近似求解方程的方法。方法使用函数$\displaystyle f(x)$的<a href="https://zh.wikipedia.org/wiki/%E6%B3%B0%E5%8B%92%E7%BA%A7%E6%95%B0" target="_blank" rel="external">泰勒级数</a>的前面几项来寻找方程$\displaystyle f(y)=0$的根。</p><p>——维基百科</p></blockquote><p>牛顿法可以通过迭代逼近的方法，求得函数$f(x)=0$的解。</p><p><img src="http://jackie-image.oss-cn-hangzhou.aliyuncs.com/18-6-7/69557176.jpg" alt=""></p><ol><li>先初始化某个点$x_0$，对该点求导数$f’(x_0)$，可以得到一条切线；</li><li>切线会和横轴再有一个交点$x_1$，然后再重复第一步；</li><li>直到$f(x_n)=0$</li></ol><p>通过一系列推导，我们可以得知：<br>$$<br>x_{i+1}-x_{i}=\frac{f(x^{(i)})}{f’(x^{(i)})}<br>$$<br>于是，我们可以将牛顿法用于极大似然估计，也就是求$l(\theta)$的最大值，可以看做是求$l’(\theta)=0$的解。</p><p>那么，每次迭代就可以写成：<br>$$<br>\theta^{(t+1)}=\theta^{(t)}-\frac{l’(\theta^{(t)})}{l’’(\theta^{(t)}}<br>$$<br>更一般地，可以写成：<br>$$<br>\theta^{(t+1)}=\theta^{(t)}-H^{-1}\nabla_\theta l<br>$$<br>其中，$H$是$l(\theta)$的Hessian矩阵：<br>$$<br>H_{ij}=\frac{\partial^2l}{\partial\theta_i\partial\theta_j}<br>$$<br>但这个方法有个缺点，每次迭代的时候，都需要重新计算$H^{-1}$，虽然牛顿法对函数$f$有很多要求和限制，但对于logistic函数而言，足够有效。</p>]]></content>
      
      <categories>
          
          <category> 牛顿法 </category>
          
      </categories>
      
      
    </entry>
    
    <entry>
      <title>5. 二分类问题</title>
      <link href="/blog-cs229/2018/06/05/5.%20%E4%BA%8C%E5%88%86%E7%B1%BB%E9%97%AE%E9%A2%98/"/>
      <url>/blog-cs229/2018/06/05/5.%20%E4%BA%8C%E5%88%86%E7%B1%BB%E9%97%AE%E9%A2%98/</url>
      <content type="html"><![CDATA[<p>在二分类问题中，输出$y\in {0, 1}$。同样的，我们也可以用线性拟合来尝试解决二分类问题（如下图左），但数据点比较异常时，容易出现下图右这样的情况：</p><p><img src="http://jackie-image.oss-cn-hangzhou.aliyuncs.com/18-6-5/41455819.jpg" alt=""></p><p>一般，在二分类问题中，我们会选用『<em>logistic</em>函数』来拟合（因为形状像<em>S</em>，又称为『<em>sigmoid</em>函数』）：<br>$$<br>h_\theta (x)=g(\theta^Tx)=\frac{1}{1+e^{-\theta^Tx}}<br>$$<br><em>logistic</em>函数$g(z)=1/(1+e^{-z})​$的形状如下：<br><img src="http://jackie-image.oss-cn-hangzhou.aliyuncs.com/18-6-5/53166407.jpg" alt=""><br>可以定义<br>$$<br>\begin{align}<br>P(y=1|x;\theta)&amp; =h_\theta (x) \\<br>P(y=0|x;\theta)&amp; =1-h_\theta(x)<br>\end{align}<br>$$<br>于是：<br>$$<br>P(y|x;\theta)=h_\theta(x)^y(1-h_\theta(x))^{(1-y)}<br>$$<br>进行极大似然估计：<br>$$<br>L(\theta)=P(y|x;\theta)=\prod_{i=1}^mP(y^{(i)}|x^{(i)};\theta)=\prod_{i=1}^mh_\theta(x^{(i)})^{y^{(i)}}(1-h_\theta(x^{(i)}))^{(1-y^{(i)})}<br>$$<br>为了计算方便，定义<br>$$<br>\begin{align}<br>l(\theta)&amp;=log(L(\theta))\\<br>&amp;=\sum_{i=1}^mlog(P(y^{(i)}|x^{(i)};\theta))\\<br>&amp;=\sum_{i=1}^m(y^{(i)}\cdot log(h_\theta(x^{(i)}))+(1-y^{(i)})\cdot log(1-h_\theta(x^{(i)})))<br>\end{align}<br>$$<br>利用梯度上升进行求解：<br>$$<br>\theta := \theta + \alpha \nabla_\theta l(\theta)<br>$$<br>其中<br>$$<br>\nabla_{\theta_j} l(\theta)=\frac{\partial}{\partial\theta_j}l(\theta)=\sum_{i=1}^m(y^{(i)}-h_\theta(x^{(i)}))\cdot x_j^{(i)}\\<br>\theta_j:=\theta_j+\alpha \cdot \sum_{i=1}^m(y^{(i)}-h_\theta(x^{(i)}))\cdot x_j^{(i)}<br>$$<br>最终的梯度上升结果几乎与线性拟合中的梯度下降结果一样。</p>]]></content>
      
      <categories>
          
          <category> 分类 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 分类 </tag>
            
            <tag> 二分类 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>4. 线性模型的概率解释</title>
      <link href="/blog-cs229/2018/06/05/4.%20%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B%E7%9A%84%E6%A6%82%E7%8E%87%E8%A7%A3%E9%87%8A/"/>
      <url>/blog-cs229/2018/06/05/4.%20%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B%E7%9A%84%E6%A6%82%E7%8E%87%E8%A7%A3%E9%87%8A/</url>
      <content type="html"><![CDATA[<p>关于：为何在进行线性回归时，选择用最小二乘拟合（距离的平方和）来进行，而不是选用其他的模型（比如三次方或四次方）？</p><p>我们更新一下假设函数，使之变为：<br>$$<br>y^{(i)} = \theta^Tx^{(i)} + \varepsilon^{(i)}<br>$$<br>其中，$\varepsilon^{(i)}$是误差项，表示未捕获的特征（unmodeled effects），比如房子存在壁炉也影响价格，或者其他的一些随机噪音（random noise）。</p><p>一般，会假设误差项$\varepsilon^{(i)} \sim N(0, \sigma^2)$（满足正态分布），也就是：<br>$$<br>P(\varepsilon^{(i)})=\frac{1}{\sqrt{2\pi}\sigma}exp(-\frac{(\varepsilon^{(i)})^2}{2\sigma^2})<br>$$<br>关于为什么假设正态分布的解释：</p><ol><li>便于数学运算；</li><li>很多独立分布的变量之间相互叠加后会趋向于正态分布（中心极限定理），在大多数情况下能成立</li></ol><p>所以，$y^{(i)}$的后验分布：<br>$$<br>P(y^{(i)}|x^{(i)};\theta)=\frac{1}{\sqrt{2\pi}\sigma}exp(-\frac{(y^{(i)}-\theta^Tx^{(i)})^2}{2\sigma^2}) \sim N(\theta^Tx^{(i)}, \sigma^2)<br>$$</p><p>之后，进行极大似然估计（maximum likelihood estimation）：$max L(\theta)$，即选择合适的$\theta$，使得$y^{(i)}$对于$x^{(i)}$出现的概率最高（有一些存在即合理的感觉），其中$L(\theta)$的定义如下：<br>$$<br>L(\theta)=P(y|x;\theta)=\prod_{i=1}^mP(y^{(i)}|x^{(i)};\theta)=\prod_{i=1}^m\frac{1}{\sqrt{2\pi}\sigma}exp(-\frac{(y^{(i)}-\theta^Tx^{(i)})^2}{2\sigma^2})<br>$$<br>那么，为了计算方便，我们定义：<br>$$<br>l(\theta) = log(L(\theta))=\sum_{i=1}^mlog(P(y^{(i)}|x^{(i)};\theta))=m\cdot log(\frac{1}{\sqrt{2\pi}\sigma})-\sum_{i=1}^m\frac{(y^{(i)}-\theta^Tx^{(i)})^2}{2\sigma^2}<br>$$<br>于是，极大似然估计变为最小化：<br>$$<br>\sum_{i=1}^m\frac{(y{(i)}-\theta^Tx{(i)})2}{2\sigma2}<br>$$<br>也即之前线性回归所需进行最小二乘拟合的$J(\theta)$。</p>]]></content>
      
      <categories>
          
          <category> 线性回归 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 线性回归 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>3. 过拟合&amp;局部加权回归</title>
      <link href="/blog-cs229/2018/06/04/3.%20%E8%BF%87%E6%8B%9F%E5%90%88&amp;%E5%B1%80%E9%83%A8%E5%8A%A0%E6%9D%83%E5%9B%9E%E5%BD%92/"/>
      <url>/blog-cs229/2018/06/04/3.%20%E8%BF%87%E6%8B%9F%E5%90%88&amp;%E5%B1%80%E9%83%A8%E5%8A%A0%E6%9D%83%E5%9B%9E%E5%BD%92/</url>
      <content type="html"><![CDATA[<h2 id="欠拟合和过拟合"><a href="#欠拟合和过拟合" class="headerlink" title="欠拟合和过拟合"></a>欠拟合和过拟合</h2><p>对于之前房价的例子，假设只有一个特征size。</p><p>假如，我们只用简单的线性拟合（$\theta_0+\theta_1x_1$，$x_1$表示size），最终拟合结果会变一条直线，就可能产生下图最左边的结果，我们称之为『欠拟合』。</p><p>当我们尝试用二次曲线来拟合（$\theta_0+\theta_1x_1+\theta_2x_1^2$，可以假设$x_2=x_1^2$，再进行线性拟合），就可能产生中间的结果。</p><p>但如果再继续增加曲线的复杂度，对于下图这种五个样本的例子，假如我们用一个五次曲线来拟合它（$\theta_0+\theta_1x1+\theta_2x1^2+\cdots+\theta_5x_1^5$）就会精确拟合所有数据，产生右图的结果，我们称之为『过拟合』。</p><p><img src="http://jackie-image.oss-cn-hangzhou.aliyuncs.com/18-6-4/71073333.jpg" alt=""></p><h2 id="局部加权回归（Locally-Weighted-Regression）"><a href="#局部加权回归（Locally-Weighted-Regression）" class="headerlink" title="局部加权回归（Locally Weighted Regression）"></a>局部加权回归（Locally Weighted Regression）</h2><p>局部加权回归，是一种特定的非参数学习方法。</p><p>什么叫非参数学习方法，首先，简单了解一下『参数化学习方法』(parametric learning algorithm)，是一种参数固定的学习方法，如上所示。而『非参数化学习方法』（non-parametric learning algorithm）则不固定参数，参数的个数会随着训练集数量而增长。</p><p>我们回顾一下，线性拟合中，我们的目标是找到合适的参数$\theta$，使得最小化$\sum_i(Y^{(i)} - \theta^TX^{(i)})^2$。</p><p>而『局部线性拟合』，则是在某个局部区域A进行线性拟合，目标是最小化$\sum_iw^{(i)}(Y^{(i)} - \theta^TX^{(i)})^2$，其中权重$$w^{(i)} = exp(-\frac{(x^{(i))}-x)^2}{2})$$，当然，权重公式是可替换的。</p><p><img src="http://jackie-image.oss-cn-hangzhou.aliyuncs.com/18-6-4/30285296.jpg" alt=""></p><p>我们观察一下$w^{(i)}$的形状，当数据$x^{(i)}$靠近$x$时，其权重就会较大，那么对目标函数的贡献就会大一些；而数据远离$x$的时候，权重就会较小，贡献就会较小。这样做，目标函数就会更关注$x$附近的数据点，从而达到局部的目的。</p><p><img src="http://jackie-image.oss-cn-hangzhou.aliyuncs.com/18-6-4/78196882.jpg" alt=""></p><p>当然，可以调整权重函数，常用的另一个权重函数：$$w^{(i)} = exp(-\frac{(x^{(i))}-x)^2}{2 \tau^2 })$$（波长函数），$\tau$越大，波形越平缓，局部性越差。</p><p><img src="http://jackie-image.oss-cn-hangzhou.aliyuncs.com/18-6-4/54967961.jpg" alt=""></p><p>但问题在于，当训练数据较大时，该方法的代价会很高。每要预测一个值，就需要重新进行一次局部线性拟合。</p>]]></content>
      
      <categories>
          
          <category> 课堂笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 机器学习 </tag>
            
            <tag> 课堂笔记 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>2. 线性回归</title>
      <link href="/blog-cs229/2018/06/03/2.%20%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/"/>
      <url>/blog-cs229/2018/06/03/2.%20%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/</url>
      <content type="html"><![CDATA[<p>首先引入一些后面会用到的定理：</p><p><strong>定义1</strong>：定义函数$f: \Bbb R^{m \times n} \mapsto \Bbb R$，$A \in \Bbb R^{m \times n}$，定义<br>$$<br>\nabla_Af(A)=<br>    \begin{bmatrix}<br>    \frac{\partial f}{\partial A_{11}} &amp; \cdots &amp; \frac{\partial f}{\partial A_{1n}}\\<br>    \vdots &amp; \ddots &amp; \vdots \\<br>    \frac{\partial f}{\partial A_{m1}} &amp; \cdots &amp; \frac{\partial f}{\partial A_{mn}}<br>    \end{bmatrix}<br>$$<br><strong>定义2</strong>：矩阵的迹（Trace）：如果$A \in R^{n\times n}$方阵，那么$A$的迹，是$A$对角线元素之和<br>$$<br>tr A = \sum_{i=1}^nA_{ii}<br>$$<br><strong>定理1</strong>：$tr AB = tr BA$</p><p><strong>定理2</strong>：$tr ABC = tr CAB = tr BCA$</p><p><strong>定理3</strong>：$f(A)=tr AB \Rightarrow \nabla_Af(A)=B^T$</p><p><strong>定理4</strong>：$trA = tr A^T$</p><p><strong>定理5</strong>：$a \in R \Rightarrow tr a=a$</p><p><strong>定理6</strong>：$\nabla_AtrABA^TC=CAB+C^TAB^T$</p><h2 id="线性回归"><a href="#线性回归" class="headerlink" title="线性回归"></a>线性回归</h2><h4 id="一些符号的改写"><a href="#一些符号的改写" class="headerlink" title="一些符号的改写"></a>一些符号的改写</h4><p><a href="http://jackieanxis.coding.me/2018/06/03/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95/" target="_blank" rel="external">上一篇博客</a>提到，梯度下降的每一步，对某个参数$\theta_i$，执行：<br>$$<br>\displaystyle \theta_i:=\theta_i - \alpha\frac{\partial}{\partial \theta_i}J(\theta)<br>$$<br>那么，$h_\theta(x)$的所有参数$\theta$可以表示成一列向量：<br>$$<br>\theta = \left[<br>    \begin{array}{c}<br>    \theta_0\\<br>    \theta_1\\<br>    \vdots\\<br>    \theta_n<br>    \end{array}<br>\right] \in R^{n+1}<br>$$</p><p>我们可以定义：<br>$$<br>\nabla_\theta J = \left[<br>    \begin{array}{c}<br>    \frac{\partial}{\partial \theta_0}J\\<br>    \frac{\partial}{\partial \theta_1}J\\<br>    \vdots\\<br>    \frac{\partial}{\partial \theta_n}J<br>    \end{array}<br>\right] \in R^{n+1}<br>$$</p><p>梯度下降过程可以表示成：<br>$$<br>\theta:=\theta - \alpha\nabla_\theta J<br>$$<br>其中，$\theta$和$\nabla_\theta J$都说是n+1维向量。</p><p>对于训练集中所有的输入${x^{(1)}},x^{(2)},…,x^{(m)}$，其中<br>$$<br>x^{(i)} = \left[<br>    \begin{array}{c}<br>    1\\<br>    x_1^{(i)}\\<br>    \vdots\\<br>    x_n^{(i)}\\<br>    \end{array}<br>\right] \in R^{n+1}<br>$$</p><p>$h(x)=h_{\theta}(x)=\theta_0+\theta_1x_1+\theta_2x_2+…+\theta_nx_n$，可以表示成向量：<br>$$<br>\left[<br>    \begin{array}{c}<br>    h_\theta(x^{(1)})\\<br>    h_\theta(x^{(2)})\\<br>    \vdots\\<br>    h_\theta(x^{(m)})\\<br>    \end{array}<br>\right] = \left[<br>    \begin{array}{c}<br>    (x^{(1)})^T\theta\\<br>    (x^{(2)})^T\theta\\<br>    \vdots\\<br>    (x^{(m)})^T\theta\\<br>    \end{array}<br>\right] = \left[<br>    \begin{array}{c}<br>    (x^{(1)})^T\\<br>    (x^{(2)})^T\\<br>    \vdots\\<br>    (x^{(m)})^T<br>    \end{array}<br>\right] \cdot \theta = X \cdot \theta<br>$$</p><p>而<br>$$<br>Y = \left[<br>    \begin{array}{c}<br>    y^{(1)}\\<br>    y^{(2)}\\<br>    \vdots\\<br>    y^{(m)}<br>    \end{array}<br>\right]<br>$$<br>于是，<br>$$<br>J(\theta) = \frac{1}{2}\sum_{i=1}^{m}(h(x^{(i)} - y^{(i)})^2)=\frac{1}{2}(X \cdot \theta - Y)^T(X \cdot \theta - Y)<br>$$</p><h4 id="推导过程"><a href="#推导过程" class="headerlink" title="推导过程"></a>推导过程</h4><p>关于梯度下降法，可以直接简化为求梯度为0的位置，即求$\nabla_\theta J(\theta) = \vec{0}$</p><p>首先，简化：<br>$$<br>\begin{align}<br>\nabla_\theta J(\theta) &amp; = \nabla_\theta\frac{1}{2}(X \cdot \theta - Y)^T(X \cdot \theta - Y)\\<br>&amp; =\frac{1}{2}\nabla_\theta tr(\theta^TX^TX\theta - \theta^TX^TY - Y^TX\theta + Y^TY)\\<br>&amp; =\frac{1}{2}[\nabla_\theta tr(\theta\theta^TX^TX) - \nabla_\theta tr(Y^TX\theta) - \nabla tr(Y^TX\theta)]<br>\end{align}<br>$$<br>其中，第一项：<br>$$<br>\begin{align}<br>\nabla_\theta tr(\theta\theta^TX^TX) &amp; = \nabla_\theta tr(\theta I \theta^TX^TX) &amp;\text{定理6, set: $\theta =^{set} A, I = B, X^TX=C$}\\<br>&amp; = X^TX\theta I + X^TX\theta I &amp; \text{$CAB+C^TAB^T$}\\<br>&amp; = X^TX\theta + X^TX\theta<br>\end{align}<br>$$<br>第二项和第三项：<br>$$<br>\nabla_\theta tr(Y^TX\theta) = X^TY\\<br>(定理3，set:Y^TX = B, \theta = A)<br>$$<br>所以：<br>$$<br>\nabla_\theta J(\theta) = X^TX\theta - X^TY = 0\\<br>\Rightarrow X^TX\theta = X^TY\\<br>$$<br>最后解得：<br>$$<br>\theta = (X^TX)^{(-1)}X^TY<br>$$<br>当然，以上的解是有限制的，只有当$X^TX$满秩时，才能够求逆。</p><p>如果非满秩，说明方程数量不够，也就是当需要n个参数时，却不够n个输入样本。</p>]]></content>
      
      <categories>
          
          <category> 课堂笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 机器学习 </tag>
            
            <tag> 课堂笔记 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>14. 经验风险最小化（ERM）</title>
      <link href="/blog-cs229/2018/06/03/14.%E7%BB%8F%E9%AA%8C%E9%A3%8E%E9%99%A9%E6%9C%80%E5%B0%8F%E5%8C%96%EF%BC%88ERM%EF%BC%89/"/>
      <url>/blog-cs229/2018/06/03/14.%E7%BB%8F%E9%AA%8C%E9%A3%8E%E9%99%A9%E6%9C%80%E5%B0%8F%E5%8C%96%EF%BC%88ERM%EF%BC%89/</url>
      <content type="html"><![CDATA[<p>就线性分类模型而言，可以将其表示为：<br>$$<br>h_\theta(x)=g(\theta^Tx), \\<br>g(z) = 1\lbrace z \geq 0 \rbrace<br>$$<br>其中，训练集表示为：<br>$$<br>S=\lbrace (x^{(i)}, y^{(i)}) \rbrace _ {i = 1} ^ m, (x^{(i)}, y^{(i)}) \sim {\cal D}<br>$$<br>这里假设了训练数据都是独立同分布的。</p><p>那么，我们认为，这个线性分类器的<strong>训练误差</strong>就可以表示为它分类错误的样本比例：<br>$$<br>{\hat{\varepsilon}}(h_\theta) = {\hat{\varepsilon}}_s(h_\theta) = \frac{1}{m}\sum_{i=1}^m1\lbrace h_\theta (x^{(i)}) \neq y^{(i)} \rbrace<br>$$<br>在这里，我们把训练误差也称为<strong>风险</strong>（risk），由此我们导出了经验风险最小化。</p><h3 id="经验风险最小化（Empirical-Risk-Minimization，ERM）"><a href="#经验风险最小化（Empirical-Risk-Minimization，ERM）" class="headerlink" title="经验风险最小化（Empirical Risk Minimization，ERM）"></a>经验风险最小化（Empirical Risk Minimization，ERM）</h3><p>经验风险最小化，最终导出一组参数，能够使得训练误差最小：<br>$$<br>{\hat{\theta}} = \arg \min {\hat{\varepsilon}}_s(h_\theta)<br>$$</p><p>我们再定义一个假设类${\cal{H}} = \lbrace h_\theta, \theta \in {\Bbb R}^{n+1} \rbrace$，它是所有假设的集合。在线性分类中，也就是所有线性分类器的集合。</p><p>那么，我们可以重新定义一次ERM：<br>$$<br>{\hat h} = \mathop{\arg \min}_{h \in {\cal H}} {\hat \varepsilon}(h)<br>$$<br>对上述公式的直观理解就是：从假设类中选取一个假设，使得训练误差最小。我们这里用了$\hat{h}$表示估计，因为毕竟不可能得到最好的假设，只能得到对这个最好的假设的估计。</p><p>但这仍然不是目标，我们的目标是使得<strong>泛化误差 Generalization Error</strong>最小化，也即新的数据集上分类错误的概率：<br>$$<br>\varepsilon(h)=P_{(x,y) \sim {\cal D}}(h(x) \neq y)<br>$$<br>接下去，为了证明：</p><ul><li><p>（1）${\hat \varepsilon} \approx \varepsilon$，训练误差近似于泛化误差（理解为，泛化误差和训练误差之间的差异存在上界）</p></li><li><p>（2）ERM输出的泛化误差$\varepsilon({\hat h})$存在上界；</p></li></ul><p>我们引出两个引理：</p><blockquote><p>联合界引理（Union Bound）</p><p>$A_1, A_2, \ldots , A_k$是k个事件，他们之间并不一定是独立分布的，有：<br>$$<br>P(A_1 \cup \ldots \cup A_k) \leq P(A_1) + \dots + P(A_k)<br>$$</p><p>Hoeffding不等式（Hoeffding Inequality）</p></blockquote>]]></content>
      
      <categories>
          
          <category> ERM </category>
          
      </categories>
      
      
        <tags>
            
            <tag> ERM </tag>
            
            <tag> 经验风险 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>1. 监督学习&amp;梯度下降法</title>
      <link href="/blog-cs229/2018/06/03/1.%20%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0&amp;%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95/"/>
      <url>/blog-cs229/2018/06/03/1.%20%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0&amp;%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95/</url>
      <content type="html"><![CDATA[<h2 id="监督学习"><a href="#监督学习" class="headerlink" title="监督学习"></a>监督学习</h2><h3 id="符号定义："><a href="#符号定义：" class="headerlink" title="符号定义："></a>符号定义：</h3><table><thead><tr><th>符号</th><th>意义</th></tr></thead><tbody><tr><td>$m$</td><td>训练集包含的数据个数</td></tr><tr><td>$x$</td><td>输入变量/特征（feature）</td></tr><tr><td>$y$</td><td>输出变量/目标（target）</td></tr><tr><td>$(x, y)$</td><td>一个训连样本</td></tr><tr><td>$(x^{(i)}, y^{(i)})$</td><td>第i个训练样本</td></tr></tbody></table><h3 id="监督学习的主要流程："><a href="#监督学习的主要流程：" class="headerlink" title="监督学习的主要流程："></a>监督学习的主要流程：</h3><p><img src="http://jackie-image.oss-cn-hangzhou.aliyuncs.com/18-6-3/4328727.jpg" alt=""></p><h3 id="线性回归"><a href="#线性回归" class="headerlink" title="线性回归"></a>线性回归</h3><p>以预测房价为例，我们的目标是导出一个函数（即假设），根据房子的特征（比如大小、卧室数量等等）来预测房价，那么：</p><ul><li>输入（特征）：$x_1, x_2, …$（比如大小、卧室数量等等）</li><li>输出（目标）：$y$（房价）</li><li><p>假设：$h(x)=h_{\theta}(x)=\theta_0+\theta_1x_1+\theta_2x_2+…+\theta_nx_n$，用于预测房价，其中$\theta_i$是参数，$n$是特征数量</p><p>为了方便，可以将假设写成：$h(x)=\sum_{i=0}^n\theta_ix_i=\theta^Tx​$</p></li></ul><p>此时，学习函数（Learning Algorithm）的目标就是找到合适的参数$\theta$，使之能够导出『合理』的假设$h(x)$，这里我们将『合理』理解为：$h_\theta(x)$（假设）和$y$（目标）之间的差距最小，也即：</p><p>$$<br>\displaystyle \min_{\theta}\frac{1}{2}\sum_{i=1}^m(h_\theta(x)^{(i)}-y_{(i)})^2<br>$$<br>这里的$\frac{1}{2}$是为了简化之后的计算。</p><p>我们定义$$\displaystyle J(\theta)=\frac{1}{2}\sum_{i=1}^m(h_\theta(x)^{(i)}-y_{(i)})^2$$，那么我们的目标就是去选取合适的$\theta$，以最小化$J(\theta)$。</p><h2 id="梯度下降法"><a href="#梯度下降法" class="headerlink" title="梯度下降法"></a>梯度下降法</h2><h3 id="搜索算法（梯度下降）"><a href="#搜索算法（梯度下降）" class="headerlink" title="搜索算法（梯度下降）"></a>搜索算法（梯度下降）</h3><p>目的：不断改变$\theta$，从而来减少$J(\theta)$。</p><p>原理：每次都往下降最快的地方走，从而找到一个局部最优解。</p><p><img src="http://jackie-image.oss-cn-hangzhou.aliyuncs.com/18-6-3/2339619.jpg" alt=""></p><p>一般会初始化$\vec{\theta}=\vec{0}$，然后每次都沿着梯度方向走，以保证每次都往下降最快的地方走：<br>$$<br>\displaystyle \theta_i:=\theta_i - \alpha\frac{\partial}{\partial \theta_i}J(\theta)<br>$$<br>其中，$:=$表示赋值操作，$\alpha$为步长。</p><p>对于某个训练样本$(x, y)$<br>$$\displaystyle \frac{\partial}{\partial \theta_i}J(\theta) = \frac{\partial}{\partial \theta_i}(\frac{1}{2}(h_\theta(x)-y)^2)$$<br>    $$\displaystyle = 2 \times \frac{1}{2}(h_\theta(x)-y)\frac{\partial}{\partial \theta_i}(h_\theta(x)-y)$$<br>    $$\displaystyle = (h_\theta(x)-y)\frac{\partial}{\partial \theta_i}(\theta_0x_0+…+\theta_nx_n-y)$$<br>    $$\displaystyle =(h_\theta(x)-y) \times x_i$$</p><p>那么，$\theta_i:=\theta_i - \alpha (h_\theta(x)-y) \times x_i$</p><h3 id="批量梯度下降法（Batch-Gradient-Descent）"><a href="#批量梯度下降法（Batch-Gradient-Descent）" class="headerlink" title="批量梯度下降法（Batch Gradient Descent）"></a>批量梯度下降法（Batch Gradient Descent）</h3><p>批量梯度下降法，使用的是所有训练样本的平均梯度：</p><p>$$<br>\displaystyle \theta_i:=\theta_i - \alpha \frac{1}{m} \sum_{j=1}^m(h_\theta(x^{(j)})-y^{(j)}) \times x_i^{(j)}<br>$$</p><p>但每次下降都需要遍历所有样本，效率较低，具体过程可能如下：</p><p><img src="http://jackie-image.oss-cn-hangzhou.aliyuncs.com/18-6-3/49403960.jpg" alt=""></p><h3 id="随机梯度下降法（Stochastic-Gradient-Descent）"><a href="#随机梯度下降法（Stochastic-Gradient-Descent）" class="headerlink" title="随机梯度下降法（Stochastic Gradient Descent）"></a>随机梯度下降法（Stochastic Gradient Descent）</h3><p>又称为『增量梯度下降法』</p><p>对每个样本$(x_{(j)}, y_{(j)})$进行：<br>$$<br>\displaystyle \theta_i:=\theta_i - \alpha (h_\theta(x^{(j)})-y^{(j)}) \times x_i^{(j)}<br>$$</p><p>直到收敛</p><p>这时，每次梯度下降只遍历一个样本，具体过程可能如下：</p><p><img src="http://jackie-image.oss-cn-hangzhou.aliyuncs.com/18-6-3/85884341.jpg" alt=""></p>]]></content>
      
      <categories>
          
          <category> 梯度下降 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 梯度下降 </tag>
            
            <tag> 线性回归 </tag>
            
        </tags>
      
    </entry>
    
  
  
</search>
