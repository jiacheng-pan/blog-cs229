[{"title":"12. SVM（三）核函数","url":"/blog-cs229/2018/07/22/12. SVM（三）核函数/","content":"\n\n\n在SVM(二)中，我们看到了如下的表示形式：\n$$\nW(\\alpha)=\\sum_{i=1}\\alpha_i-\\frac{1}{2}\\sum_{i=1}\\sum_{j=1}\\alpha_i\\alpha_jy_iy_j(x_i \\cdot x_j)\n$$\n这里，内积$(x_i \\cdot x_j)$就是最简单的核函数的形式。一般核函数会被写成$\\langle x^{(i)}, x^{(j)} \\rangle$的形式。\n\n有时候，我们会将一些特征转换到高维空间上，就像我们在之前的[3. 过拟合&局部加权回归](https://jackieanxis.github.io/blog-cs229/2018/06/04/3.%20%E8%BF%87%E6%8B%9F%E5%90%88&%E5%B1%80%E9%83%A8%E5%8A%A0%E6%9D%83%E5%9B%9E%E5%BD%92/)中提到的，比如特征$x$表示的是房屋面积，我们需要预测房子是否会在6个月内被卖出，我们有时候会将这个特征映射成如下的形式：\n$$\nx \\rightarrow \\begin{bmatrix}\nx \\\\\\\nx^2 \\\\\\\nx^3 \\\\\\\nx^4\n\\end{bmatrix} = \\phi(x)\n$$\n原先的特征的内积形式$\\langle x^{(i)}, x^{(j)} \\rangle$会被写成$\\langle \\phi(x^{(i)}), \\phi(x^{(j)}) \\rangle$，而且往往$\\phi(x)$会有很高的维度。因为在很多情况下，计算$\\phi(x)$会有很高的代价，或者表示$\\phi(x)$需要很高的代价，但是光是计算内核则可能代价较小。\n\n比如：假如有两个输入：$x, z \\in \\Bbb R^n$，核函数被定义为：\n$$\n\\begin{align}\nk(x, z) = (x^T z)^2 &= (\\sum_{i=1}^nx_iz_i)(\\sum_{j=1}^nx_jz_j) \\\\\\\n&=\\sum_{i=1}^n\\sum_{j=1}^n(x_ix_j)(z_iz_j) \\\\\\\n&= \\phi(x)^T\\phi(z)\n\\end{align}\n$$\n假如需要表示成高维向量，那么$\\phi(x)$是一个$n \\times n$维的向量，如果$n = 3$：\n$$\n\\phi(x) = \\begin{bmatrix}\nx_1x_1 \\\\\\\nx_1x_2 \\\\\\\nx_1x_3 \\\\\\\nx_2x_1 \\\\\\\n\\vdots \\\\\\\nx_3x_3\n\\end{bmatrix}\n$$\n所以，计算$\\phi(x)$的时间复杂度就达到了$O(n^2)$，而计算核函数仅仅需要计算$x^Tz$，复杂度为$O(n)$。\n\n接下去我们为这个核函数增加常数项：\n$$\nk(x,z)=(x^Tz+c)^2\n$$\n那么：\n$$\n\\phi(x) = \\begin{bmatrix}\nx_1x_1 \\\\\\\nx_1x_2 \\\\\\\nx_1x_3 \\\\\\\nx_2x_1 \\\\\\\n\\vdots \\\\\\\nx_3x_3 \\\\\\\n\\sqrt{2c}x_1 \\\\\\\n\\sqrt{2c}x_2 \\\\\\\n\\sqrt{2c}x_3 \\\\\\\nc\n\\end{bmatrix}\n$$\n更一般的：\n$$\nk(x, z)=(x^Tz+c)^d\n$$\n\n\n有了核函数，即可替换SVM中的内积$\\langle x^{(i)}, x^{(j)} \\rangle$，比如常用的高斯核：\n$$\nk(x,z)=\\exp(-\\frac{||x-z||^2}{2\\sigma^2})\n$$\n有了核函数，相当于把数据从原始空间转换到了高位空间，很多数据，在一维空间往往是线性不可分的，但是到了高维空间会变成可分的：\n\n![](http://o6vut8vrh.bkt.clouddn.com/18-7-23/7489488.jpg)\n\n\n\n## 核函数的合法性\n\n如何判断一个核函数是合法的呢？判断依据是：是否存在函数$\\phi$，使得$k(x,z)$能够被写成$\\langle \\phi(x), \\phi(z) \\rangle$。\n\n> **定理**：如果核函数合法，那么其对应的核矩阵（kernel matrix）是半正定的。\n\n核矩阵指的是矩阵$K \\in \\Bbb R^{m\\times m}$，其中$K_{ij}=k(x^{(i)}, x^{(j)})$。半正定的意思是，对于任意向量$z$，都存在$z^TKz \\geq 0$，证明如下：\n$$\n\\begin{align}\nz^TKz &= \\sum_i\\sum_jz_iK_{ij}z_j \\\\\\ \n&= \\sum_i\\sum_jz_i\\phi_(x^{(i)})^T\\phi_(x^{(j)})z_j \\\\\\ \n&= \\sum_i\\sum_jz_i\\cdot \\sum_k\\phi_(x^{(i)})_k\\underbrace{\\phi_(x^{(j)})_k}_{向量第k项} \\cdot z_j \\\\\\ \n&= \\sum_k\\sum_i\\sum_jz_i\\cdot \\phi_(x^{(i)})_k\\phi_(x^{(j)})_k \\cdot z_j \\\\\\ \n&= \\sum_k(\\sum_iz_i\\phi(x^{(i)}))^2 \\geq 0\n\\end{align}\n$$\n事实上，上面的定理的逆命题也一样成立，总结起来：\n\n>**Merce定理**：给定核函数$k(x, z)$，那么$k(x, z)$合法（也即$\\exists \\phi, k(x,z)=\\phi(x)^T\\phi(z)$），当且仅当，对所有的$\\lbrace x^{(1)}, \\ldots, x^{(m)} \\rbrace$，核矩阵$K \\in \\Bbb R^{m\\times m}$是一个对称的半正定矩阵。\n\n\n\n","tags":["核函数"],"categories":["SVM"]},{"title":"11. SVM（二）最优间隔分类器","url":"/blog-cs229/2018/07/03/11. SVM（二）最优间隔分类器/","content":"\n\n\n**最优间隔分类器**（*Optimal Margin Classifier*）。其目标是使得最小几何间隔最大化（[10. SVM（一）概念](https://jackieanxis.github.io/blog-cs229/2018/07/02/10.%20SVM%EF%BC%88%E4%B8%80%EF%BC%89%E6%A6%82%E5%BF%B5/)）：\n$$\n\\text{目标(1):} \\\\\\\n\\max_{w, b} \\gamma \\\\\\\n\\text{ s.t. } y^{(i)} \\cdot ((\\frac{w}{||w||})^T \\cdot x^{(i)}+\\frac{b}{||w||}) \\geq \\gamma, i=1,\\ldots,n\n$$\n我们知道，$\\hat{\\gamma} = \\frac{\\gamma}{||w||}$，所以上面的目标可以等同于：\n$$\n\\text{目标(2):} \\\\\\\n\\max_{w, b} \\frac{\\hat{\\gamma}}{||w||} \\\\\\\n\\text{ s.t. }y^{(i)} \\cdot (w^T \\cdot x^{(i)}+b) \\geq \\hat{\\gamma}, i=1,\\ldots,n\n$$\n为了最大化上述值，我们有两种策略。\n\n1. 增大$\\hat{\\gamma}$\n2. 减小$||w||$\n\n针对第一种可能，我们要证明其无效性。假如，我们增大$\\hat{\\gamma}$到$\\hat{\\gamma}_1 := \\lambda \\hat{\\gamma}$，因为$\\hat{\\gamma}=y(w^Tx+b)$，可以视作$w_1:=\\lambda w, b_1 = \\lambda b$。所以，此时\n$$\n\\frac{\\hat{\\gamma_1}}{||w_1||}=\\frac{\\lambda \\hat{\\gamma}}{||\\lambda w||} = \\frac{\\hat{\\gamma}}{||w||} \\\\\\\n$$\n没有发生任何改变，所以第一条策略不可行。于是，我们可以固定$\\hat{\\gamma}=1$\n\n此时，上述目标(2)可以表述成：\n$$\n\\text{目标(3):} \\\\\\\n\\min_{w, b} \\frac{1}{2}||w|^2 \\\\\\\n\\text{ s.t. }y^{(i)} \\cdot (w^T \\cdot x^{(i)}+b) \\geq 1, i=1,\\ldots,n\n$$\n\n## 拉格朗日乘子法（Lagrange Multiplier）\n\n为了解决上述的**凸优化问题**，我们引入拉格朗日乘子法*Lagrange Multiplier*来解决这个问题。\n\n我们首先来看看**凸优化问题**的定义：\n$$\n\\min_wf(w) \\\\\\\n\\text{s.t. }g_i(w) \\leq 0, h_i(w) =0\n$$\n构建拉格朗日乘子：\n$$\n{\\cal L}(w, \\alpha, \\beta) = f(w)+\\sum_i\\alpha_ig_i(w)+\\sum_i\\beta_ih_i(w)\n$$\n定义：\n$$\n\\theta_p(w) = \\max_{\\alpha_i>0, \\beta}{\\cal L}(w, \\alpha, \\beta)\n$$\n观察$\\theta_p(w)$：\n\n1. 如果$g_i(w)>0$，那么$\\theta_p(w)=+\\infty$（因为$\\alpha$可以取任意大值）。\n2. 如果$h_i(w) \\neq 0$，那么$\\theta_p(w)=+\\infty$（因为$\\beta$可以取$+\\infty/-\\infty$）。\n\n所以，在满足约束的情况下，$\\theta_p(w)=f(w)$，$\\min_w \\theta_p(w)=\\min_w f(w)$，因为使得${\\cal L}(w, \\alpha, \\beta)$最大的方法，就是其他所有项全是0。那么，可以得出这样的结论：\n$$\n\\theta_p(w)=\\begin{cases}\nf(w), &\\text{满足约束} \\\\\\\n\\infty, &\\text{不满足约束}\n\\end{cases}\n$$\n因此，在满足条件的情况下，$\\min_w\\theta_p(w)$等价于$min_wf(w)$。\n\n我们将最优间隔分类器的目标重新表示一下：\n$$\np^\\ast =\\min_{w, b}\\max_\\alpha {\\cal L(w, \\alpha, b)} \\\\\\\n{\\cal L}(w, \\alpha, b) = \\frac{1}{2}||w||^2+\\sum_i\\alpha_i(1-y^{(i)}(w^T x^{(i)}+b))\n$$\n其中，直接忽略了$h_i(w)=0$的约束，而$g_i(w,b)=1-y^{(i)}(w^Tx^{(i)}+b) \\leq 0, f(w)=\\frac{1}{2}||w||^2$\n\n\n\n## 对偶问题（Dual Problem）\n\n一般来说，将原始问题转化成对偶问题来求解。一是因为对偶问题往往比较容易求解，二是因为对偶问题引入了核函数，方便推广到非线性分类的情况。\n\n我们看到，之前的原始问题，是\n$$\np^\\ast =\\min_{w, b}\\max_\\alpha {\\cal L}(w, \\alpha, b)\n$$\n\n那么，定义其对偶问题：\n\n$$\nl^\\ast =\\max_\\alpha\\min_{w,b}{\\cal L}(w, \\alpha, b)\n$$\n\n接下去，我们求解对偶问题：\n\n先求解$\\min_{w,b}{\\cal L}(w, \\alpha, b)$：\n\n分别求偏导，使其等于0，导出最小值：\n$$\n\\begin{align}\n& \\nabla_w{\\cal L}(w, \\alpha, b) =w-\\sum_{i=1}\\alpha_iy^{(i)}x^{(i)}=0 \\\\\\\n& \\nabla_b{\\cal L}(w, \\alpha, b) =\\sum_{i=1}\\alpha_iy^{(i)}=0\n\\end{align}\n$$\n\n得到：\n\n$$\nw =\\sum_{i=1}\\alpha_iy^{(i)}x^{(i)} \\\\\\\n\\sum_{i=1}\\alpha_iy^{(i)} = 0\n$$\n\n代入${\\cal L}(w, \\alpha, b)$，就可以得到最小值：\n\n$$\n\\begin{align}\n{\\cal L}(w, \\alpha, b) &= \\frac{1}{2}||w||^2+\\sum_i\\alpha_i(1-y^{(i)}(w^T x^{(i)}+b)) \\\\\\\n\\min_{w, b}{\\cal L}(w, \\alpha, b) &=\\underbrace{\\sum_{i=1}\\alpha_i-\\frac{1}{2}\\sum_{i=1}\\sum_{j=1}\\alpha_i\\alpha_jy_iy_j(x_i \\cdot x_j)}_{W(\\alpha)}\n\\end{align}\n$$\n\n于是，我们的对偶问题简化到了对$W(\\alpha)$最大化：\n$$\n\\max_\\alpha W(\\alpha) \\\\\\\n\\text{s.t. }\\alpha_i \\geq 0, \\sum_iy_i\\alpha_i=0\n$$\n\n假设，我们解得的对偶问题的解为：$\\alpha^\\ast =[\\alpha_1^\\ast ,\\alpha_2^\\ast , \\ldots, \\alpha_m^\\ast ]$，那么最终原始问题的解可以表示成：\n\n$$\nw^\\ast =\\sum_{i=1}\\alpha_i^\\ast y^{(i)}x^{(i)}\n$$\n\n在原始问题中，还有$b$未得到解决。我们先来观察一下约束项：\n$$\ng_i(w,b)=1-y{(i)}(w^Tx^{(i)}+b) \\leq 0\n$$\n![](http://o6vut8vrh.bkt.clouddn.com/18-7-4/9336603.jpg)\n\n我们知道，在数据中，只有少数的几个数据点，他们的函数距离为1（最小），也即$g_i(w,b)=0$，如图所示。\n\n在整个数据集中，只有这些数据点对约束超平面起了作用，这些数据点被称为支持向量（*support vector*），其对应的$\\alpha_i^\\ast  \\neq 0$，而其他不是支持向量的数据点，没有对约束超平面起作用，其$\\alpha_i^\\ast =0$。\n\n此时，我们已经得到了$w^\\ast $，而$b^\\ast $的计算如下，找到一个数据点，其$\\alpha_j^\\ast  \\neq 0$(也就是支持向量，其函数间隔为1)，我们就能得到：\n\n$$\ny^{(j)}(w^{*T}x^{(j)}+b^\\ast )=1\n\\Rightarrow\nb^\\ast =y^{(j)}-\\sum_{i=1}\\alpha_i^\\ast y^{(i)}(x^{(i)} \\cdot x^{(j)})\n$$\n\n","tags":["拉格朗日乘子法"],"categories":["SVM"]},{"title":"10. SVM（一）概念","url":"/blog-cs229/2018/07/02/10. SVM（一）概念/","content":"\n*SVM*，指的是支持向量机（*support vector machines*）。\n\n支持向量机，假设数据是线性可分的，那么我们就能找到一个超平面，将数据分成两类。但是一旦线性可分，我们就可能找到无数的超平面，都可以将数据分成两类：\n\n![](http://o6vut8vrh.bkt.clouddn.com/18-7-3/83131877.jpg)\n\n但是很明显，上图中虽然*a, c*都对数据进行了有效的分割。但很明显，都不如*b*分割的好。\n\n我们可以用“间隔”这个概念来定义这个超平面（在二维上是线）对数据的分割优劣。在分类正确的情况下，间隔越大，我们认为对数据的分类越好。\n\n我们的目标是得到数据的分类：$y \\in \\lbrace -1, +1 \\rbrace$。\n\n这个超平面，则可以表示成$w^Tx+b$，其中$w=[\\theta_1, \\ldots, \\theta_n]^T, b=\\theta_0$。这个超平面可以表达成一个$n+1$维向量。\n\n判别函数：\n$$\ng(z)=\\begin{cases}\n+1, & \\text{如果$z\\geq0$} \\\\\\\n-1, & \\text{otherwise}\n\\end{cases}\n$$\n假设则可以表示成：$h_{w,b}(x)=g(w^Tx+b)$\n\n\n\n## 间隔\n\n### 函数间隔（functional margin）\n\n某个超平面$(w,b)$和训练样本$(x^{(i)}, y^{(i)})$之间的函数间隔被表示成：\n$$\n\\hat{\\gamma}^{(i)}=y^{(i)}(w^Tx^{(i)}+b)\n$$\n于是，我们可以知道：\n\n1. 当$y^{(i)}=1$，于是我们想获得更大的函数间隔（这是我们的目标），就需要使得$w^Tx^{(i)}+b \\gg 0$\n2. 相反，当$y^{(i)}=-1$，我们想获得更大的函数间隔，就需要使得$w^Tx^{(i)}+b \\ll 0$\n\n并且，很明显，只有当函数间隔$\\hat{\\gamma}>0$时，分类结果是正确的。\n\n最后，超平面与数据集$\\lbrace (x^{(1)}, y^{(1)}), (x^{(2)}, y^{(2)}), \\ldots \\rbrace$之间的函数间隔，被定义为所有函数间隔中的最小值：\n$$\n\\hat{\\gamma}=\\min_i\\hat{\\gamma}^{(i)}\n$$\n\n\n### 几何间隔（geometric margin）\n\n![](http://o6vut8vrh.bkt.clouddn.com/18-7-3/34659777.jpg)\n\n从点$(x^{(i)}, y^{(i)})$出发，对超平面做垂线，得到点D，我们知道他们之间的距离，就是该超平面到数据点$(x^{(i)}, y^{(i)})$的几何间隔。\n\n经过推导，D的坐标可以表示为：\n$$\nx^{(i)}-\\gamma^{(i)}\\frac{w}{||w||}\n$$\n又因为，D在超平面$w^Tx+b=0$上，所以：\n$$\n\\begin{align}\n& w^T(x^{(i)}-\\gamma^{(i)}\\frac{w}{||w||})+b=0 \\\\\\\n& \\Rightarrow w^Tx^{(i)}+b=\\gamma^{(i)} \\cdot \\frac{w^Tw}{||w||}=\\gamma^{(i)} \\cdot ||w|| \\\\\\\n& \\Rightarrow \\gamma^{(i)}=(\\frac{w}{||w||})^T \\cdot x^{(i)}+\\frac{b}{||w||}\n\\end{align}\n$$\n加上正负分类的判断：\n$$\n\\gamma^{(i)}=y^{(i)} \\cdot ((\\frac{w}{||w||})^T \\cdot x^{(i)}+\\frac{b}{||w||})\n$$\n我们可以看到，几何间隔跟函数间隔之间存在如下的关系：\n$$\n\\hat{\\gamma}^{(i)} = \\frac{\\gamma^{(i)}}{||w||}\n$$\n\n\n同样的，超平面与数据集$\\lbrace (x^{(1)}, y^{(1)}), (x^{(2)}, y^{(2)}), \\ldots \\rbrace$之间的几何间隔，被定义为所有几何间隔中的最小值：\n$$\n\\gamma=\\min_i\\gamma^{(i)}\n$$\n最后，我们导出**最优间隔分类器**（*Optimal Margin Classifier*）问题：选择$w, b$，最大化$\\gamma$，同时满足$\\forall(x^{(i)}, y^{(i)})$，$ y^{(i)} \\cdot ((\\frac{w}{||w||})^T \\cdot x^{(i)}+\\frac{b}{||w||}) \\geq \\gamma$（所有数据点的几何间隔都大于该最小几何间隔）。\n\n目前为止，已经是SVM问题的一个简化版本。\n\n","tags":["几何间隔"],"categories":["SVM"]},{"title":"9. 生成学习算法的例子","url":"/blog-cs229/2018/06/29/9. 生成学习算法的例子/","content":"\n## 例一：高斯判别分析和logistic函数\n\n我们来看一个例子，对于一个高斯判别分析问题，根据贝叶斯：\n$$\n\\begin{align}\np(y=1|x) &= \\frac{p(x|y=1)p(y=1)}{p(x)} \\\\\\\n&= \\frac{p(x|y=1)p(y=1)}{p(x|y=0)p(y=0)+p(x|y=1)p(y=1)}\n\\end{align}\n$$\n在这里，我们提出几个假设：\n\n1. $p(y)$是均匀分布的，也就是$p(y=1)=p(y=0)$\n2. $x$的条件概率分布（$p(x|y=0)$和$p(x|y=1)$）满足高斯分布。\n\n考虑二维的情况：\n\n![image-20180630164349595](http://o6vut8vrh.bkt.clouddn.com/2018-06-30-084350.png)\n\n蓝色数据表达的是$p(x|y=0)$的分布，红色数据表达的是$p(x|y=1)$的分布，两条蓝色和红色的曲线分别是它们的概率密度曲线。\n\n而灰色的曲线则表示了$p(y=1|x)$的概率密度曲线。\n\n假设$p(x|y=0) \\sim N(\\mu_0, \\sigma_0)$，$p(x|y=1) \\sim N(\\mu_1, \\sigma_1)$，而$p(y)$均匀分布那么：\n$$\n\\begin{align}\np(y=1|x) &= \\frac{N(\\mu_0,\\sigma_0)}{N(\\mu_0,\\sigma_0)+N(\\mu_1,\\sigma_1)} \\\\\\ \n&= \\cdots \\\\\\\n&= \\frac{1}{1+\\frac{\\sigma_0}{\\sigma_1}exp(2\\sigma_1^2(x-\\mu_0)^2-2\\sigma_0^2(x-\\mu_1)^2}\n\\end{align}\n$$\n事实上，这条曲线跟我们之前见过的*logistic*曲线非常像，特别是当我们假设$\\sigma_0=\\sigma_1$的时候，就是一条*logistic*曲线。\n\n我们有如下的推广结论：\n$$\n{\\begin{cases}\np(x|y=1) \\sim Exp Family(\\eta_1) \\\\\\\np(x|y=0) \\sim Exp Family(\\eta_0)\n\\end{cases}} \\Rightarrow p(y=1|x)是logistic函数\n$$\n但这个命题的逆命题并不成立，故而我们知道，*logistic*所需要的假设更少（无需假设$x$的条件概率分布），鲁棒性更强。而生成函数因为对数据的分布做出了假设，所以需要的数据量会少于*logstic*回归，我们需要在两者之间进行权衡。\n\n\n\n## 例二：垃圾邮件分类（1）\n\n这里我们会用朴素贝叶斯（Naive Bayes）来解决垃圾邮件分类问题（$y\\in \\lbrace 0, 1 \\rbrace$）。\n\n首先对邮件进行建模，生成特征向量如下：\n$$\nx=\n\\begin{bmatrix}\n0 \\\\\\\n0 \\\\\\\n0 \\\\\\\n\\vdots \\\\\\\n1 \\\\\\\n\\vdots\n\\end{bmatrix}\n\\begin{matrix}\na \\\\\\\nadvark \\\\\\\nausworth \\\\\\\n\\vdots \\\\\\\nbuy \\\\\\\n\\vdots\n\\end{matrix}\n$$\n这是一个类似于词频向量的特征向量，我们有一个50000个词的词典，如果邮件中出现了某个词汇，那么其在向量中对应的位置就会被标记为1，否则为0。\n\n我们的目标是获取，垃圾邮件和非垃圾邮件的特征分别是怎么样的，也即$p(x|y)$。$x={\\lbrace 0, 1 \\rbrace}^n, y \\in \\lbrace 0, 1 \\rbrace$，这里我们的词典中词汇数量是50000，所以$n=50000$，特征向量$x$会有$2^{50000}$种可能，需要$2^{50000}-1$个参数。\n\n我们假设$x_i|y$之间相互独立(虽然假设各个单词的出现概率相互独立不是很合理，但是即便这样，朴素贝叶斯的效果依旧不错)，根据朴素贝叶斯，我们得到：\n$$\np(x_1, x_2, \\ldots, x_{50000}|y)=p(x_1|y)p(x_2|y) \\cdots p(x_{50000}|y)\n$$\n单独观察$p(x_j|y=1)​$：\n$$\np(x_j|y=1) = p(x_j=1|y=1)^{x_j}p(x_j=0|y=1)^{1-x_j}\n$$\n给定三个参数：\n$$\n\\begin{align}\n\\phi_{j|y=1} &= p(x_j=1|y=1) \\\\\\\n\\phi_{j|y=0} &= p(x_j=1|y=0) \\\\\\\n\\phi_y &= p(y = 1)\n\\end{align}\n$$\n故：\n$$\n\\begin{align}\np(x_j|y=1) &= \\phi_{j|y=1}^{x_j}(\\phi_y - \\phi_{j|y=1})^{1-x_j}\\\\\\\np(x_j|y=0) &= \\phi_{j|y=0}^{x_j}(1-\\phi_y + \\phi_{j|y=0})^{1-x_j} \\\\\\\np(x_j|y) &= p(x_j|y=1)^yp(x_j|y=0)^{1-y} \\\\\\\np(y) &= \\phi_y^y(1-\\phi_y)^{1-y}\n\\end{align}\n$$\n按照上个博客[8. 生成学习算法的概念](https://jackieanxis.github.io/blog-cs229/2018/06/29/8.%20%E7%94%9F%E6%88%90%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%9A%84%E6%A6%82%E5%BF%B5/)中所述，我们会选用联合概率分布的极大似然来导出最优解：\n$$\nl(\\phi_y,\\phi_{j|y=1},\\phi_{j|y=0}=\\prod_{i=1}^mp(x^{(i)},y^{(i)})=\\prod_{i=1}^mp(x^{(i)}|y^{(i)})p(y^{(i)})\n$$\n\n可以解得：\n$$\n\\begin{align}\n\\phi_{j|y=1} &= \\frac{\\sum_{i=1}^m1\\lbrace x_j{(i)}=1, y^{(i)}=1 \\rbrace}{\\sum_{i=1}^m1\\lbrace y^{(i)}=1 \\rbrace}  = \\frac{统计所有包含词语j的垃圾邮件的数量}{垃圾邮件的总数}\\\\\\\n\\phi_{j|y=0} &= \\frac{\\sum_{i=1}^m1\\lbrace x_j{(i)}=1, y^{(i)}=0 \\rbrace}{\\sum_{i=1}^m1\\lbrace y^{(i)}=0 \\rbrace} = \\frac{统计所有包含词语j的非垃圾邮件的数量}{非垃圾邮件的总数} \\\\\\\n\\phi_y &= \\frac{\\sum_{i=1}^m1\\lbrace y^{(i)}=1 \\rbrace}{m} = \\frac{垃圾邮件的数量}{邮件的总数}\n\\end{align}\n$$\n通过以上的公式，我们已经可以完全推得$p(x_1, x_2, \\ldots, x_{50000}|y)$。\n\n\n\n### Laplace平滑\n\n假设，训练集中，我们重来没有碰到过\"*NIPS*\"这个词汇，假设我们词典中包含这个词，位置是30000，也就是说：\n$$\n\\begin{align}\np(x_{30000}=1|y=1) &= 0\\\\\\\np(x_{30000}=0|y=1) &= 0\n\\end{align} \\\\\\\n\\Downarrow \\\\\\\np(x|y=1) =\\prod_{i=1}^{50000}p(x_i|y=1)=0 \\\\\\\np(x|y=0) =\\prod_{i=1}^{50000}p(x_i|y=0)=0\n$$\n故而在分类垃圾邮件时：\n$$\n\\begin{align}\np(y=1|x) &= \\frac{p(x|y=1)p(y=1)}{p(x)} \\\\\\\n&=\\frac{p(x|y=1)p(y=1)}{p(x|y=0)p(y=0)+p(x|y=1)p(y=1)} \\\\\\\n&= \\frac{0}{0+0}\n\\end{align}\n$$\n所以，我们提出$p(x_{30000}=1|y=1) = 0$这样的假设不够好。\n\n*Laplace*平滑就是来帮助解决这个问题的。\n\n举例而言，在计算：\n$$\n\\phi_y=p(y=1)=\\frac{\\text{numof(1)}}{\\text{numof(0)}+\\text{numof(1)}}\n$$\n其中，$\\text{numof(1)}$表示的是，被分类为1的训练集中数据个数。\n\n在*Laplace*平滑中，我们会采取如下策略:\n$$\n\\phi_y=p(y=1)=\\frac{\\text{numof(1)}+1}{\\text{numof(0)}+1+\\text{numof(1)}+1}\n$$\n比如，A球队在之前的五场比赛里面都输了，我们预测下一场比赛赢的概率：\n$$\np(y=1)=\\frac{0+1}{0+1+5+1}=\\frac{1}{7}\n$$\n而不是简单的认为（没有*Laplace*平滑）是0。\n\n推广而言，在多分类问题中，$y\\in\\lbrace1, \\ldots, k \\rbrace$，那么：\n$$\np(y=j) = \\frac{\\sum_{i=1}^m1\\lbrace y^{(i)} = j \\rbrace+1}{m+k}\n$$\n\n\n\n## 例三：垃圾邮件分类（2）\n\n之前的垃圾分类模型里面，我们对邮件提取的特征向量是：\n$$\nx=[1,0,0,\\ldots,1,\\ldots]^T\n$$\n这种模型，我们称之为多元伯努利事件模型（Multivariate Bernoulli Event Model）。\n\n现在，我们换一种特征向量提取方式，将邮件的特征向量表示为：\n$$\nx=[x_1,x_2,\\ldots,x_j,\\ldots]^T\n$$\n$x_j$表示词汇$j$在邮件中出现的次数。上述的特征向量也就是词频向量了。这种模型，我们称为多项式事件模型（Multinomial Event Model）。\n\n对联合概率分布$p(x,y)$进行极大似然估计，得到如下的参数：\n$$\n\\begin{align}\n\\phi_{k|y=1} &= p(x_j=k|y=1) = \\frac{C_{x=k}+1}{C_{y=1}+n} \\\\\\\n\\phi_{k|y=0} &=p(x_j=k|y=0) = \\frac{C_{x=k}+1}{C_{y=0}+n} \\\\\\\n\\phi_{y} &= p(y=1) = \\frac{C_{y=1}+1}{C_{y=1}+1+C_{y=0}+1}\n\\end{align}\n$$\n其中：\n\n$n$表示词典中词汇的数量，也就是特征向量的长度；\n$$\nC_{x=k}=\\sum_{i=1}^m(1\\lbrace y^{(i)}=1 \\rbrace \\sum_{j=1}^{n_i}1 \\lbrace x_j^{(i)} = k \\rbrace)\n$$\n表示在训练集中，所有垃圾邮件中词汇$k$出现的次数（并不是邮件的次数，而是词汇的次数）；\n$$\nC_{y=1}=\\sum_{i=1}^n(1\\lbrace y^{(i)} = 1 \\rbrace \\cdot n_i)\n$$\n表示训练集中垃圾邮件的所有词汇总长；\n$$\nC_{y=0}=\\sum_{i=1}^n(1\\lbrace y^{(i)} = 0 \\rbrace \\cdot n_i)\n$$\n表示训练集中非垃圾邮件的所有词汇总长；","tags":["拉普拉斯平滑"],"categories":["生成学习算法"]},{"title":"8. 生成学习算法的概念","url":"/blog-cs229/2018/06/29/8. 生成学习算法的概念/","content":"\n生成学习算法，英文为*Generative Learning Algorithm*。\n\n我们之前看到的都是判别学习算法（*Discriminative Learning Algorithm*）。判别学习算法可以分成两种：\n\n1. 学得$p(y|x)$，比如之前的线性模型\n2. 学得一个假设$h_\\theta (x) = \\lbrace 0, 1 \\rbrace$，比如二分类问题\n\n上面都是根据特征$x$，来对输出$y$进行建模，也许$y$是一个连续的值，也可能是离散的，比如类别。\n\n那么，生成学习算法（*Generative Learning Algorithm*）刚好跟判别学习算法（*Discriminative Learning Algorithm*）相反，其用输出$y$对$x$进行建模，也就是：**学得$p(x|y)$**。\n\n**详细解释**：\n\n对于一个二分类或多分类问题，生成学习算法，在给定样本所述的类别的条件下，会对其样本特征建立一个概率模型。举例而言，在给定癌症是良性或者是恶性的条件下，生成模型会对该癌症的特征的概率分布进行建模。\n\n根据朴素贝叶斯，\n$$\np(y=1|x)=\\frac{p(x|y=1) p(y=1)}{p(x)}\n$$\n因为给定了$x$，所以$p(x)$可以视作1，也即\n$$\np(y=1|x)=p(x|y=1) p(y=1)\n$$\n\n\n## 高斯判别算法\n\n*Gaussian Discriminant Algorithm*\n\n对特征满足高斯分布的二分类问题进行建模。\n\n假设$x \\in \\Bbb R^n$，且$p(x|y)$满足高斯分布，因为$x$往往是一个特征向量，故而这里的高斯分布是一个多元高斯分布（*multivariate Gaussian）*。\n\n> 多元高斯分布，$z \\sim N(\\mu, \\Sigma)$。\n> $$\n> p(z)=\\frac{1}{\\sqrt{(2\\pi)^n|\\Sigma|}}exp(-\\frac{1}{2}(z-\\mu)^T\\Sigma^{-1}(z-\\mu))\n> $$\n> $z$是一个$n$维向量，$\\mu$则是均值向量，$\\Sigma$是协方差矩阵：$E[(z-\\mu)(z-\\mu)^T]$。\n\n对于一个二分类判别问题，正则响应函数为$\\phi$，那么\n$$\np(y)=\\phi^y(1-\\phi)^{1-y}\n$$\n且\n$$\np(x|y=0) \\sim N(\\mu_0, \\Sigma) \\\\\\\np(x|y=1) \\sim N(\\mu_1, \\Sigma)\n$$\n(这里假设了，对于$y=0$和$y=1$，是两个不同均值，但协方差矩阵相同的多元高斯分布，所以这也是该模型的限制之一)\n\n我们写出对数似然函数：\n$$\n\\begin{align}\nl(\\phi, \\mu_0, \\mu_1, \\Sigma) &= log\\prod_{i=1}^mp(x^{(i)}, (y^{(i)})) \\\\\\\n&= log\\prod_{i=1}^mp(x^{(i)}| (y^{(i)}))p(y^{(i)})\n\\end{align}\n$$\n\n对比一下之前[二分类问题](https://jackieanxis.github.io/blog-cs229/2018/06/05/5.%20%E4%BA%8C%E5%88%86%E7%B1%BB%E9%97%AE%E9%A2%98/)中的对数似然估计函数：\n$$\nl(\\theta) = \\sum_{i=1}^mlog(P(y^{(i)}|x^{(i)};\\theta))\n$$\n这里，我们使用了联合概率（*joint likelihood*）: $p(x^{(i)}, (y^{(i)}))$，而在之前的判别模型里，我们用的是条件概率（*conditional likelihood*）: $P(y^{(i)}|x^{(i)};\\theta)$。因为，在判别模型中，特征$x$是给定的，所以用条件概率来进行极大似然估计；而在生成模型中，特征$x$不给定，所以需要用联合概率来进行极大似然估计。\n\n\n\n然后，我们进行极大似然估计，得到：\n$$\n\\phi=\\frac{1}{m}\\sum_{i=1}^my^{(i)}=\\frac{1}{m}\\sum_{i=1}^m 1\\lbrace y^{(i)} = 1 \\rbrace\n$$\n也就是分类标签为1的训练样本的比例。\n\n再看$\\mu_0, \\mu_1$：\n$$\n\\mu_0=\\underbrace{\\sum_{i=1}^m 1\\lbrace y^{(i)} = 0 \\rbrace \\cdot x^{(i)}}_{(1)} /\\underbrace{ \\sum_{i=1}^m 1\\lbrace y^{(i)} = 0 \\rbrace}_{(2)} \\\\\\\n\\mu_1=\\sum_{i=1}^m 1\\lbrace y^{(i)} = 1 \\rbrace \\cdot x^{(i)} / \\sum_{i=1}^m 1\\lbrace y^{(i)} = 1 \\rbrace\n$$\n式(1)表达了分类为0的样本中，特征$x$的和；式(2)表达了分类为0的样本个数；故而$\\mu_0$表达了样本中，分类为0的样本的特征的均值。同理得到$\\mu_1$。\n\n\n\n我们再次回顾上面的高斯判别算法，事实上，高斯判别算法，假设了特征$x$满足了多元高斯分布，利用极大似然估计，对不同类别的特征$x$的分布进行了建模。\n\n下节课我们将会探讨其他的关于生成学习算法的案例。","tags":["高斯判别算法"],"categories":["生成学习算法"]},{"title":"7. 广义线性模型","url":"/blog-cs229/2018/06/09/7. 广义线性模型/","content":"\n\n\n广义线性模型，英文名为**Generalized Linear Model**，简称GLM。\n\n之前，涉及到两种的两种模型：\n1. 线性拟合模型，假设了$P(y|x;\\theta)$是高斯分布\n2. 二分类问题，假设了$P(y|x;\\theta)$满足伯努利分布\n\n但以上两者知识一种更广泛的，被称为『指数分布族』（The Exponential Family）的特例。\n\n\n\n## 指数分布族\n\n$$\nP(y;\\eta)=b(y)exp(\\eta^TT(y)-a(\\eta))\n$$\n\n可以被表示为以上形式的分布，都是指数分布族的某个特定分布，给定$a, b, T$，就可以定义一个概率分布的集合，以$\\eta$为参数，就可以得到不同的概率分布。\n\n在广义线性模型中，会假设$\\eta=\\theta^Tx$，也就是$\\eta$和特征$x$线性相关。\n\n\n\n## 伯努利分布\n\n首先，我们给出$y=1$的概率：\n$$\nP(y=1;\\phi)=\\phi\n$$\n于是：\n$$\n\\begin{align}\nP(y;\\phi)\n\t&= \\phi^y(1-\\phi)^T\\\\\\\n\t&= exp(log(\\phi^T(1-\\phi^T)))\\\\\\\n\t&= exp(ylog(\\phi)+(1-y)log(1-\\phi))\\\\\\\n\t&= exp(log\\frac{\\phi}{1-\\phi} \\cdot y + log(1-\\phi))\n\\end{align}\n$$\n比较我们上面的概率形式和指数分布族的标准形式，可以得到：\n$$\n\\begin{cases}\n\\eta &= log\\frac{\\phi}{1-\\phi}, \\text{于是} \\phi=\\frac{1}{1+e^{-\\eta}}\\\\\\\na(\\eta) &= -log(1-\\phi)=log(1+e^\\eta)\\\\\\\nT(y) &= y\\\\\\\nb(y) &= 1\n\\end{cases}\n$$\n\n这里的$\\phi$一般会被称为正则响应函数（*canonic response function*）：\n$$\ng(\\eta) = E[y|\\eta]=\\frac{1}{1+e^{-\\eta}}\n$$\n相对的，正则关联函数（*canonic link function*）则是$g^{-1}$。\n\n## 高斯分布\n\n$$\nN(\\mu,\\sigma^2)=\\frac{1}{\\sqrt{2\\pi}}exp(-\\frac{1}{2}(y-\\mu)^2)\n$$\n\n这里，出于简洁考虑，假设$\\sigma=1$，经过一系列化简后，可以表示成：\n$$\n\\frac{1}{\\sqrt{2\\pi}} \\cdot exp(-\\frac{1}{2}y^2) \\cdot exp(\\mu y-\\frac{1}{2}\\mu^2)\n$$\n那么，\n$$\n\\begin{cases}\n\\eta &= \\mu\\\\\\\na(\\eta) &= \\frac{1}{2}\\mu^2=\\frac{1}{2}\\eta^2\\\\\\\nT(y) &= y\\\\\\\nb(y) &= \\frac{1}{\\sqrt{2\\pi}} \\cdot exp(-\\frac{1}{2}y^2)\n\\end{cases}\n$$\n\n\n\n## 多项式分布\n\n#### 建模\n\n在二项分布中，$y\\in \\lbrace 1, 2 \\rbrace$\n\n而多项式分布，$y \\in \\lbrace 1,\\cdots, k \\rbrace$\n\n一般会被用来进行邮件分类或者进行病情分类等等\n\n我们假设\n$$\nP(y=i)=\\phi_i\n$$\n也即，邮件属于$i$类的概率是$\\phi_i$，是关于特征$x$的一个函数。\n\n那么，可以用$k$个参数来建模多项式分布\n$$\nP(y)=\\prod_{i=1}^k\\phi_i^{1\\lbrace y=i \\rbrace}\n$$\n\n其中，$1 \\lbrace \\cdots \\rbrace$的含义为，检验$\\cdots$是否为真命题，若为真命题，则取1，否则取0。\n\n因为所有概率和为1，所以最后一个参数\n$$\n\\begin{align}\n\\phi_k &= 1-\\sum_{j=1}^{k-1}\\phi_j \\\\\\\n1 \\lbrace y=k \\rbrace &=1-\\sum_{j=1}^{k-1}1 \\lbrace y=j \\rbrace\n\\end{align}\n$$\n经过化简，也可以表示成：\n$$\nP(y)=exp[\\sum_{i=1}^{k-1}(log(\\frac{\\phi_i}{\\phi_k}) \\cdot 1\\lbrace y=i \\rbrace )] + log(\\phi_k)\n$$\n故而\n$$\n\\eta = \\begin{bmatrix}\nlog(\\frac{\\phi_1}{\\phi_k}) \\\\\\\n\\vdots \\\\\\\nlog(\\frac{\\phi_{k-1}}{\\phi_k})\n\\end{bmatrix} \\in \\Bbb R^{k-1}\n$$\n$$\na(\\eta) = -log(\\phi_k)\n$$\n$$\nT(y)= \\begin{bmatrix}\n1 \\lbrace y=1 \\rbrace \\\\\\\n\\vdots \\\\\\\n1 \\lbrace y=k-1 \\rbrace\n\\end{bmatrix} \\in (0, 1)^{k-1}\n$$\n$$\nb(y) = 1\n$$\n\n根据$\\eta$可得：\n$$\n\\phi_i = e^{\\eta_i} \\cdot \\phi_k\n$$\n又因为：\n$$\n\\sum_{i=1}^{k}\\phi_i=\\sum_{i=1}^k\\phi_ke^{\\eta_i}=1\n$$\n故而：\n$$\n\\phi_k = \\frac{1}{\\sum_{i=1}^ke^{\\eta_i}}=\\frac{1}{e^{\\eta_k}+\\sum_{i=1}^{k-1}e^{\\eta_i}} = \\frac{1}{1+\\sum_{i=1}^{k-1}e^{\\eta_i}}\n$$\n所以：\n$$\n\\begin{align}\n\\phi_i &= e^{\\eta_i} \\cdot \\phi_k \\\\\\\n&= \\frac{e^{\\eta_i}}{1 + \\sum_{j=1}^{k-1}e^{\\eta_j}} \\\\\\\n&= \\frac{e^{\\theta_i^Tx_i}}{1 + \\sum_{j=1}^{k-1}e^{\\theta_j^Tx_j}}\n\\end{align}\n$$\n上述函数，被称为『softmax』函数，这个函数的作用经常用于进行归一化。\n\n经过上述步骤，假设函数可以被写成如下形式：\n$$\nh_\\theta(x)=\n\\left[\n\\begin{array}{c}\n1\\lbrace y=1 \\rbrace  \\\\\\\n\\vdots \\\\\\\n1\\lbrace y=k-1 \\rbrace\n\\end{array} | x;\\theta\n\\right]=\n\\begin{bmatrix}\n\\phi_1\\\\\\\n\\vdots\\\\\\\n\\phi_{k-1}\n\\end{bmatrix}\n$$\n\n#### 回归\n\n在经过上述推导，当我们有一堆训练集（$(x^{(1)}, y^{(1)}), \\cdots, (x^{(m)}, y^{(m)})$）用于训练的时候，则可以进行极大似然估计：\n$$\nL(\\theta) = \\prod_{i=1}^mP(y^{(i)} | x^{(i)};\\theta) = \\prod_{i=1}^m\\prod_{j=1}^k\\phi_j^{1\\lbrace y^{(i)}=j \\rbrace }\n$$\n","tags":["指数分布族"],"categories":["广义线性模型"]},{"title":"6. 牛顿法","url":"/blog-cs229/2018/06/07/6. 牛顿法/","content":"\n\n\n> **牛顿法**（英语：Newton's method）又称为**牛顿-拉弗森方法**（英语：Newton-Raphson method），它是一种在实数域和复数域上近似求解方程的方法。方法使用函数$\\displaystyle f(x)$的[泰勒级数](https://zh.wikipedia.org/wiki/%E6%B3%B0%E5%8B%92%E7%BA%A7%E6%95%B0)的前面几项来寻找方程$\\displaystyle f(y)=0$的根。\n>\n> ——维基百科\n\n牛顿法可以通过迭代逼近的方法，求得函数$f(x)=0$的解。\n\n![](http://o6vut8vrh.bkt.clouddn.com/18-6-7/69557176.jpg)\n\n1. 先初始化某个点$x_0$，对该点求导数$f'(x_0)$，可以得到一条切线；\n2. 切线会和横轴再有一个交点$x_1$，然后再重复第一步；\n3. 直到$f(x_n)=0$\n\n通过一系列推导，我们可以得知：\n$$\nx_{i+1}-x_{i}=\\frac{f(x^{(i)})}{f'(x^{(i)})}\n$$\n于是，我们可以将牛顿法用于极大似然估计，也就是求$l(\\theta)$的最大值，可以看做是求$l'(\\theta)=0$的解。\n\n那么，每次迭代就可以写成：\n$$\n\\theta^{(t+1)}=\\theta^{(t)}-\\frac{l'(\\theta^{(t)})}{l''(\\theta^{(t)}}\n$$\n更一般地，可以写成：\n$$\n\\theta^{(t+1)}=\\theta^{(t)}-H^{-1}\\nabla_\\theta l\n$$\n其中，$H$是$l(\\theta)$的Hessian矩阵：\n$$\nH_{ij}=\\frac{\\partial^2l}{\\partial\\theta_i\\partial\\theta_j}\n$$\n但这个方法有个缺点，每次迭代的时候，都需要重新计算$H^{-1}$，虽然牛顿法对函数$f$有很多要求和限制，但对于logistic函数而言，足够有效。","categories":["牛顿法"]},{"title":"5. 二分类问题","url":"/blog-cs229/2018/06/05/5. 二分类问题/","content":"\n\n\n在二分类问题中，输出$y\\in \\{0, 1\\}$。同样的，我们也可以用线性拟合来尝试解决二分类问题（如下图左），但数据点比较异常时，容易出现下图右这样的情况：\n\n![](http://o6vut8vrh.bkt.clouddn.com/18-6-5/41455819.jpg)\n\n一般，在二分类问题中，我们会选用『*logistic*函数』来拟合（因为形状像*S*，又称为『*sigmoid*函数』）：\n$$\nh_\\theta (x)=g(\\theta^Tx)=\\frac{1}{1+e^{-\\theta^Tx}}\n$$\n*logistic*函数$g(z)=1/(1+e^{-z})​$的形状如下：\n![](http://o6vut8vrh.bkt.clouddn.com/18-6-5/53166407.jpg)\n可以定义\n$$\n\\begin{align}\nP(y=1|x;\\theta)& =h_\\theta (x) \\\\\\\nP(y=0|x;\\theta)& =1-h_\\theta(x)\n\\end{align}\n$$\n于是：\n$$\nP(y|x;\\theta)=h_\\theta(x)^y(1-h_\\theta(x))^{(1-y)}\n$$\n进行极大似然估计：\n$$\nL(\\theta)=P(y|x;\\theta)=\\prod_{i=1}^mP(y^{(i)}|x^{(i)};\\theta)=\\prod_{i=1}^mh_\\theta(x^{(i)})^{y^{(i)}}(1-h_\\theta(x^{(i)}))^{(1-y^{(i)})}\n$$\n为了计算方便，定义\n$$\n\\begin{align}\nl(\\theta)&=log(L(\\theta))\\\\\\\n&=\\sum_{i=1}^mlog(P(y^{(i)}|x^{(i)};\\theta))\\\\\\\n&=\\sum_{i=1}^m(y^{(i)}\\cdot log(h_\\theta(x^{(i)}))+(1-y^{(i)})\\cdot log(1-h_\\theta(x^{(i)})))\n\\end{align}\n$$\n利用梯度上升进行求解：\n$$\n\\theta := \\theta + \\alpha \\nabla_\\theta l(\\theta)\n$$\n其中\n$$\n\\nabla_{\\theta_j} l(\\theta)=\\frac{\\partial}{\\partial\\theta_j}l(\\theta)=\\sum_{i=1}^m(y^{(i)}-h_\\theta(x^{(i)}))\\cdot x_j^{(i)}\\\\\\\n\\theta_j:=\\theta_j+\\alpha \\cdot \\sum_{i=1}^m(y^{(i)}-h_\\theta(x^{(i)}))\\cdot x_j^{(i)}\n$$\n最终的梯度上升结果几乎与线性拟合中的梯度下降结果一样。\n\n","tags":["二分类"],"categories":["分类"]},{"title":"4. 线性模型的概率解释","url":"/blog-cs229/2018/06/05/4. 线性模型的概率解释/","content":"\n关于：为何在进行线性回归时，选择用最小二乘拟合（距离的平方和）来进行，而不是选用其他的模型（比如三次方或四次方）？\n\n我们更新一下假设函数，使之变为：\n$$\ny^{(i)} = \\theta^Tx^{(i)} + \\varepsilon^{(i)}\n$$\n其中，$\\varepsilon^{(i)}$是误差项，表示未捕获的特征（unmodeled effects），比如房子存在壁炉也影响价格，或者其他的一些随机噪音（random noise）。\n\n一般，会假设误差项$\\varepsilon^{(i)} \\sim N(0, \\sigma^2)$（满足正态分布），也就是：\n$$\nP(\\varepsilon^{(i)})=\\frac{1}{\\sqrt{2\\pi}\\sigma}exp(-\\frac{(\\varepsilon^{(i)})^2}{2\\sigma^2})\n$$\n关于为什么假设正态分布的解释：\n\n1. 便于数学运算；\n2. 很多独立分布的变量之间相互叠加后会趋向于正态分布（中心极限定理），在大多数情况下能成立\n\n所以，$y^{(i)}$的后验分布：\n$$\nP(y^{(i)}|x^{(i)};\\theta)=\\frac{1}{\\sqrt{2\\pi}\\sigma}exp(-\\frac{(y^{(i)}-\\theta^Tx^{(i)})^2}{2\\sigma^2}) \\sim N(\\theta^Tx^{(i)}, \\sigma^2)\n$$\n\n之后，进行极大似然估计（maximum likelihood estimation）：$max L(\\theta)$，即选择合适的$\\theta$，使得$y^{(i)}$对于$x^{(i)}$出现的概率最高（有一些存在即合理的感觉），其中$L(\\theta)$的定义如下：\n$$\nL(\\theta)=P(y|x;\\theta)=\\prod_{i=1}^mP(y^{(i)}|x^{(i)};\\theta)=\\prod_{i=1}^m\\frac{1}{\\sqrt{2\\pi}\\sigma}exp(-\\frac{(y^{(i)}-\\theta^Tx^{(i)})^2}{2\\sigma^2})\n$$\n那么，为了计算方便，我们定义：\n$$\nl(\\theta) = log(L(\\theta))=\\sum_{i=1}^mlog(P(y^{(i)}|x^{(i)};\\theta))=m\\cdot log(\\frac{1}{\\sqrt{2\\pi}\\sigma})-\\sum_{i=1}^m\\frac{(y^{(i)}-\\theta^Tx^{(i)})^2}{2\\sigma^2}\n$$\n于是，极大似然估计变为最小化：\n$$\n\\sum_{i=1}^m\\frac{(y{(i)}-\\theta^Tx{(i)})2}{2\\sigma2}\n$$\n也即之前线性回归所需进行最小二乘拟合的$J(\\theta)$。","tags":["线性回归"],"categories":["线性回归"]},{"title":"3. 过拟合&局部加权回归","url":"/blog-cs229/2018/06/04/3. 过拟合&局部加权回归/","content":"\n\n\n## 欠拟合和过拟合\n\n对于之前房价的例子，假设只有一个特征size。\n\n假如，我们只用简单的线性拟合（$\\theta_0+\\theta_1x_1$，$x_1$表示size），最终拟合结果会变一条直线，就可能产生下图最左边的结果，我们称之为『欠拟合』。\n\n当我们尝试用二次曲线来拟合（$\\theta_0+\\theta_1x_1+\\theta_2x_1^2$，可以假设$x_2=x_1^2$，再进行线性拟合），就可能产生中间的结果。\n\n但如果再继续增加曲线的复杂度，对于下图这种五个样本的例子，假如我们用一个五次曲线来拟合它（$\\theta_0+\\theta_1x1+\\theta_2x1^2+\\cdots+\\theta_5x_1^5$）就会精确拟合所有数据，产生右图的结果，我们称之为『过拟合』。\n\n![](http://o6vut8vrh.bkt.clouddn.com/18-6-4/71073333.jpg)\n\n\n\n## 局部加权回归（Locally Weighted Regression）\n\n局部加权回归，是一种特定的非参数学习方法。\n\n什么叫非参数学习方法，首先，简单了解一下『参数化学习方法』(parametric learning algorithm)，是一种参数固定的学习方法，如上所示。而『非参数化学习方法』（non-parametric learning algorithm）则不固定参数，参数的个数会随着训练集数量而增长。\n\n我们回顾一下，线性拟合中，我们的目标是找到合适的参数$\\theta$，使得最小化$\\sum_i(Y^{(i)} - \\theta^TX^{(i)})^2$。\n\n而『局部线性拟合』，则是在某个局部区域A进行线性拟合，目标是最小化$\\sum_iw^{(i)}(Y^{(i)} - \\theta^TX^{(i)})^2$，其中权重$$w^{(i)} = exp(-\\frac{(x^{(i))}-x)^2}{2})$$，当然，权重公式是可替换的。\n\n![](http://o6vut8vrh.bkt.clouddn.com/18-6-4/30285296.jpg)\n\n我们观察一下$w^{(i)}$的形状，当数据$x^{(i)}$靠近$x$时，其权重就会较大，那么对目标函数的贡献就会大一些；而数据远离$x$的时候，权重就会较小，贡献就会较小。这样做，目标函数就会更关注$x$附近的数据点，从而达到局部的目的。\n\n![](http://o6vut8vrh.bkt.clouddn.com/18-6-4/78196882.jpg)\n\n当然，可以调整权重函数，常用的另一个权重函数：$$w^{(i)} = exp(-\\frac{(x^{(i))}-x)^2}{2 \\tau^2 })$$（波长函数），$\\tau$越大，波形越平缓，局部性越差。\n\n![](http://o6vut8vrh.bkt.clouddn.com/18-6-4/54967961.jpg)\n\n但问题在于，当训练数据较大时，该方法的代价会很高。每要预测一个值，就需要重新进行一次局部线性拟合。\n\n\n\n","tags":["课堂笔记"],"categories":["课堂笔记"]},{"title":"2. 线性回归","url":"/blog-cs229/2018/06/03/2. 线性回归/","content":"\n首先引入一些后面会用到的定理：\n\n**定义1**：定义函数$f: \\Bbb R^{m \\times n} \\mapsto \\Bbb R$，$A \\in \\Bbb R^{m \\times n}$，定义\n$$\n\\nabla_Af(A)=\n    \\begin{bmatrix}\n    \\frac{\\partial f}{\\partial A_{11}} & \\cdots & \\frac{\\partial f}{\\partial A_{1n}}\\\\\\\n    \\vdots & \\ddots & \\vdots \\\\\\\n    \\frac{\\partial f}{\\partial A_{m1}} & \\cdots & \\frac{\\partial f}{\\partial A_{mn}}\n    \\end{bmatrix}\n$$\n**定义2**：矩阵的迹（Trace）：如果$A \\in R^{n\\times n}$方阵，那么$A$的迹，是$A$对角线元素之和\n$$\ntr A = \\sum_{i=1}^nA_{ii}\n$$\n**定理1**：$tr AB = tr BA$\n\n**定理2**：$tr ABC = tr CAB = tr BCA$\n\n**定理3**：$f(A)=tr AB \\Rightarrow \\nabla_Af(A)=B^T$\n\n**定理4**：$trA = tr A^T$\n\n**定理5**：$a \\in R \\Rightarrow tr a=a$\n\n**定理6**：$\\nabla_AtrABA^TC=CAB+C^TAB^T$\n\n## 线性回归\n\n#### 一些符号的改写\n\n[上一篇博客](http://jackieanxis.coding.me/2018/06/03/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95/)提到，梯度下降的每一步，对某个参数$\\theta_i$，执行：\n$$\n\\displaystyle \\theta_i:=\\theta_i - \\alpha\\frac{\\partial}{\\partial \\theta_i}J(\\theta)\n$$\n那么，$h_\\theta(x)$的所有参数$\\theta$可以表示成一列向量：\n$$\n\\theta = \\left[\n\t\\begin{array}{c}\n\t\\theta_0\\\\\\\n\t\\theta_1\\\\\\\n\t\\vdots\\\\\\\n\t\\theta_n\n\t\\end{array}\n\\right] \\in R^{n+1}\n$$\n\n\n我们可以定义：\n$$\n\\nabla_\\theta J = \\left[\n\t\\begin{array}{c}\n\t\\frac{\\partial}{\\partial \\theta_0}J\\\\\\\n\t\\frac{\\partial}{\\partial \\theta_1}J\\\\\\\n\t\\vdots\\\\\\\n\t\\frac{\\partial}{\\partial \\theta_n}J\n\t\\end{array}\n\\right] \\in R^{n+1}\n$$\n\n梯度下降过程可以表示成：\n$$\n\\theta:=\\theta - \\alpha\\nabla_\\theta J\n$$\n其中，$\\theta$和$\\nabla_\\theta J$都说是n+1维向量。\n\n对于训练集中所有的输入${x^{(1)}},x^{(2)},…,x^{(m)}$，其中\n$$\nx^{(i)} = \\left[\n\t\\begin{array}{c}\n\t1\\\\\\\n\tx_1^{(i)}\\\\\\\n\t\\vdots\\\\\\\n\tx_n^{(i)}\\\\\\\n\t\\end{array}\n\\right] \\in R^{n+1}\n$$\n\n\n$h(x)=h_{\\theta}(x)=\\theta_0+\\theta_1x_1+\\theta_2x_2+...+\\theta_nx_n$，可以表示成向量：\n$$\n\\left[\n    \\begin{array}{c}\n    h_\\theta(x^{(1)})\\\\\\\n    h_\\theta(x^{(2)})\\\\\\\n    \\vdots\\\\\\\n    h_\\theta(x^{(m)})\\\\\\\n    \\end{array}\n\\right] = \\left[\n    \\begin{array}{c}\n    (x^{(1)})^T\\theta\\\\\\\n    (x^{(2)})^T\\theta\\\\\\\n    \\vdots\\\\\\\n    (x^{(m)})^T\\theta\\\\\\\n    \\end{array}\n\\right] = \\left[\n\t\\begin{array}{c}\n\t(x^{(1)})^T\\\\\\\n\t(x^{(2)})^T\\\\\\\n\t\\vdots\\\\\\\n\t(x^{(m)})^T\n\t\\end{array}\n\\right] \\cdot \\theta = X \\cdot \\theta\n$$\n\n而\n$$\nY = \\left[\n\t\\begin{array}{c}\n\ty^{(1)}\\\\\\\n\ty^{(2)}\\\\\\\n\t\\vdots\\\\\\\n\ty^{(m)}\n\t\\end{array}\n\\right]\n$$\n于是，\n$$\nJ(\\theta) = \\frac{1}{2}\\sum_{i=1}^{m}(h(x^{(i)} - y^{(i)})^2)=\\frac{1}{2}(X \\cdot \\theta - Y)^T(X \\cdot \\theta - Y)\n$$\n\n\n#### 推导过程\n\n关于梯度下降法，可以直接简化为求梯度为0的位置，即求$\\nabla_\\theta J(\\theta) = \\vec{0}$\n\n首先，简化：\n$$\n\\begin{align}\n\\nabla_\\theta J(\\theta) & = \\nabla_\\theta\\frac{1}{2}(X \\cdot \\theta - Y)^T(X \\cdot \\theta - Y)\\\\\\\n& =\\frac{1}{2}\\nabla_\\theta tr(\\theta^TX^TX\\theta - \\theta^TX^TY - Y^TX\\theta + Y^TY)\\\\\\\n& =\\frac{1}{2}[\\nabla_\\theta tr(\\theta\\theta^TX^TX) - \\nabla_\\theta tr(Y^TX\\theta) - \\nabla tr(Y^TX\\theta)]\n\\end{align}\n$$\n其中，第一项：\n$$\n\\begin{align}\n\\nabla_\\theta tr(\\theta\\theta^TX^TX) & = \\nabla_\\theta tr(\\theta I \\theta^TX^TX) &\\text{定理6, set: $\\theta =^{set} A, I = B, X^TX=C$}\\\\\\\n& = X^TX\\theta I + X^TX\\theta I & \\text{$CAB+C^TAB^T$}\\\\\\\n& = X^TX\\theta + X^TX\\theta\n\\end{align}\n$$\n第二项和第三项：\n$$\n\\nabla_\\theta tr(Y^TX\\theta) = X^TY\\\\\\\n(定理3，set:Y^TX = B, \\theta = A)\n$$\n所以：\n$$\n\\nabla_\\theta J(\\theta) = X^TX\\theta - X^TY = 0\\\\\\\n\\Rightarrow X^TX\\theta = X^TY\\\\\\\n$$\n最后解得：\n$$\n\\theta = (X^TX)^{(-1)}X^TY\n$$\n当然，以上的解是有限制的，只有当$X^TX$满秩时，才能够求逆。\n\n如果非满秩，说明方程数量不够，也就是当需要n个参数时，却不够n个输入样本。","tags":["课堂笔记"],"categories":["课堂笔记"]},{"title":"1. 监督学习&梯度下降法","url":"/blog-cs229/2018/06/03/1. 监督学习&梯度下降法/","content":"\n## 监督学习\n\n### 符号定义：\n\n| 符号                 | 意义                     |\n| -------------------- | ------------------------ |\n| $m$                  | 训练集包含的数据个数     |\n| $x$                  | 输入变量/特征（\u0004feature） |\n| $y$                  | 输出变量/目标（target）  |\n| $(x, y)$             | 一个训连样本             |\n| $(x^{(i)}, y^{(i)})$ | 第i个训练样本            |\n\n\n\n### 监督学习的主要流程：\n\n![](http://o6vut8vrh.bkt.clouddn.com/18-6-3/4328727.jpg)\n\n\n\n### 线性回归\n\n以预测房价为例，我们的目标是导出一个函数（即假设），根据房子的特征（比如大小、卧室数量等等）来预测房价，那么：\n- 输入（特征）：$x_1, x_2, …$（比如大小、卧室数量等等）\n- 输出（目标）：$y$（房价）\n- 假设：$h(x)=h_{\\theta}(x)=\\theta_0+\\theta_1x_1+\\theta_2x_2+...+\\theta_nx_n$，用于预测房价，其中$\\theta_i$是参数，$n$是特征数量\n\n  为了方便，可以将假设写成：$h(x)=\\sum_{i=0}^n\\theta_ix_i=\\theta^Tx​$\n\n此时，学习函数（Learning Algorithm）的目标就是找到合适的参数$\\theta$，使之能够导出『合理』的假设$h(x)$，这里我们将『合理』理解为：$h_\\theta(x)$（假设）和$y$（目标）之间的差距最小，也即：\n\n$$\n\\displaystyle \\min_{\\theta}\\frac{1}{2}\\sum_{i=1}^m(h_\\theta(x)^{(i)}-y_{(i)})^2\n$$\n这里的$\\frac{1}{2}$是为了简化之后的计算。\n\n我们定义$$\\displaystyle J(\\theta)=\\frac{1}{2}\\sum_{i=1}^m(h_\\theta(x)^{(i)}-y_{(i)})^2$$，那么我们的目标就是去选取合适的$\\theta$，以最小化$J(\\theta)$。\n\n\n\n## 梯度下降法\n\n### 搜索算法（梯度下降）\n\n目的：不断改变$\\theta$，从而来减少$J(\\theta)$。\n\n原理：每次都往下降最快的地方走，从而找到一个局部最优解。\n\n![](http://o6vut8vrh.bkt.clouddn.com/18-6-3/2339619.jpg)\n\n一般会初始化$\\vec{\\theta}=\\vec{0}$，然后每次都沿着梯度方向走，以保证每次都往下降最快的地方走：\n$$\n\\displaystyle \\theta_i:=\\theta_i - \\alpha\\frac{\\partial}{\\partial \\theta_i}J(\\theta)\n$$\n其中，$:=$表示赋值操作，$\\alpha$为步长。\n\n对于某个训练样本$(x, y)$\n$$\\displaystyle \\frac{\\partial}{\\partial \\theta_i}J(\\theta) = \\frac{\\partial}{\\partial \\theta_i}(\\frac{1}{2}(h_\\theta(x)-y)^2)$$\n\t$$\\displaystyle = 2 \\times \\frac{1}{2}(h_\\theta(x)-y)\\frac{\\partial}{\\partial \\theta_i}(h_\\theta(x)-y)$$\n\t$$\\displaystyle = (h_\\theta(x)-y)\\frac{\\partial}{\\partial \\theta_i}(\\theta_0x_0+…+\\theta_nx_n-y)$$\n\t$$\\displaystyle =(h_\\theta(x)-y) \\times x_i$$\n\n那么，$\\theta_i:=\\theta_i - \\alpha (h_\\theta(x)-y) \\times x_i$\n\n\n\n### 批量梯度下降法（Batch Gradient Descent）\n\n批量梯度下降法，使用的是所有训练样本的平均梯度：\n\n$$\n\\displaystyle \\theta_i:=\\theta_i - \\alpha \\frac{1}{m} \\sum_{j=1}^m(h_\\theta(x^{(j)})-y^{(j)}) \\times x_i^{(j)}\n$$\n\n但每次下降都需要遍历所有样本，效率较低，具体过程可能如下：\n\n![](http://o6vut8vrh.bkt.clouddn.com/18-6-3/49403960.jpg)\n\n### 随机梯度下降法（Stochastic Gradient Descent）\n\n又称为『增量梯度下降法』\n\n对每个样本$(x_{(j)}, y_{(j)})$进行：\n$$\n\\displaystyle \\theta_i:=\\theta_i - \\alpha (h_\\theta(x^{(j)})-y^{(j)}) \\times x_i^{(j)}\n$$\n\n直到收敛\n\n这时，每次梯度下降只遍历一个样本，具体过程可能如下：\n\n![](http://o6vut8vrh.bkt.clouddn.com/18-6-3/85884341.jpg)","tags":["线性回归"],"categories":["梯度下降"]}]