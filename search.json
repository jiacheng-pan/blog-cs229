[{"title":"7. 广义线性模型","url":"/blog-cs229/2018/06/09/7. 广义线性模型/","content":"\n\n\n广义线性模型，英文名为**Generalized Linear Model**，简称GLM。\n\n之前，涉及到两种的两种模型：\n1. 线性拟合模型，假设了$P(y|x;\\theta)$是高斯分布\n2. 二分类问题，假设了$P(y|x;\\theta)$满足伯努利分布\n\n但以上两者知识一种更广泛的，被称为『指数分布族』（The Exponential Family）的特例。\n\n\n\n## 指数分布族\n\n$$\nP(y;\\eta)=b(y)exp(\\eta^TT(y)-a(\\eta))\n$$\n\n可以被表示为以上形式的分布，都是指数分布族的某个特定分布，给定$a, b, T$，就可以定义一个概率分布的集合，以$\\eta$为参数，就可以得到不同的概率分布。\n\n在广义线性模型中，会假设$\\eta=\\theta^Tx$，也就是$\\eta$和特征$x$线性相关。\n\n\n\n## 伯努利分布\n\n首先，我们给出$y=1$的概率：\n$$\nP(y=1;\\phi)=\\phi\n$$\n于是：\n$$\n\\begin{align}\nP(y;\\phi)\n\t&= \\phi^y(1-\\phi)^T\\\\\\\n\t&= exp(log(\\phi^T(1-\\phi^T)))\\\\\\\n\t&= exp(ylog(\\phi)+(1-y)log(1-\\phi))\\\\\\\n\t&= exp(log\\frac{\\phi}{1-\\phi} \\cdot y + log(1-\\phi))\n\\end{align}\n$$\n比较我们上面的概率形式和指数分布族的标准形式，可以得到：\n$$\n\\begin{cases}\n\\eta &= log\\frac{\\phi}{1-\\phi}, \\text{于是} \\phi=\\frac{1}{1+e^{-\\eta}}\\\\\\\na(\\eta) &= -log(1-\\phi)=log(1+e^\\eta)\\\\\\\nT(y) &= y\\\\\\\nb(y) &= 1\n\\end{cases}\n$$\n\n这里的$\\phi$一般会被称为正则响应函数（*canonic response function*）：\n$$\ng(\\eta) = E[y|\\eta]=\\frac{1}{1+e^{-\\eta}}\n$$\n相对的，正则关联函数（*canonic link function*）则是$g^{-1}$。\n\n## 高斯分布\n\n$$\nN(\\mu,\\sigma^2)=\\frac{1}{\\sqrt{2\\pi}}exp(-\\frac{1}{2}(y-\\mu)^2)\n$$\n\n这里，出于简洁考虑，假设$\\sigma=1$，经过一系列化简后，可以表示成：\n$$\n\\frac{1}{\\sqrt{2\\pi}} \\cdot exp(-\\frac{1}{2}y^2) \\cdot exp(\\mu y-\\frac{1}{2}\\mu^2)\n$$\n那么，\n$$\n\\begin{cases}\n\\eta &= \\mu\\\\\\\na(\\eta) &= \\frac{1}{2}\\mu^2=\\frac{1}{2}\\eta^2\\\\\\\nT(y) &= y\\\\\\\nb(y) &= \\frac{1}{\\sqrt{2\\pi}} \\cdot exp(-\\frac{1}{2}y^2)\n\\end{cases}\n$$\n\n\n\n## 多项式分布\n\n#### 建模\n\n在二项分布中，$y\\in \\lbrace 1, 2 \\rbrace$\n\n而多项式分布，$y \\in \\lbrace 1,\\cdots, k \\rbrace$\n\n一般会被用来进行邮件分类或者进行病情分类等等\n\n我们假设\n$$\nP(y=i)=\\phi_i\n$$\n也即，邮件属于$i$类的概率是$\\phi_i$，是关于特征$x$的一个函数。\n\n那么，可以用$k$个参数来建模多项式分布\n$$\nP(y)=\\prod_{i=1}^k\\phi_i^{1\\lbrace y=i \\rbrace}\n$$\n\n其中，$1 \\lbrace \\cdots \\rbrace$的含义为，检验$\\cdots$是否为真命题，若为真命题，则取1，否则取0。\n\n因为所有概率和为1，所以最后一个参数\n$$\n\\begin{align}\n\\phi_k &= 1-\\sum_{j=1}^{k-1}\\phi_j \\\\\\\n1 \\lbrace y=k \\rbrace &=1-\\sum_{j=1}^{k-1}1 \\lbrace y=j \\rbrace\n\\end{align}\n$$\n经过化简，也可以表示成：\n$$\nP(y)=exp[\\sum_{i=1}^{k-1}(log(\\frac{\\phi_i}{\\phi_k}) \\cdot 1\\lbrace y=i \\rbrace )] + log(\\phi_k)\n$$\n故而\n$$\n\\eta = \\begin{bmatrix}\nlog(\\frac{\\phi_1}{\\phi_k}) \\\\\\\n\\vdots \\\\\\\nlog(\\frac{\\phi_{k-1}}{\\phi_k})\n\\end{bmatrix} \\in \\Bbb R^{k-1}\n$$\n$$\na(\\eta) = -log(\\phi_k)\n$$\n$$\nT(y)= \\begin{bmatrix}\n1 \\lbrace y=1 \\rbrace \\\\\\\n\\vdots \\\\\\\n1 \\lbrace y=k-1 \\rbrace\n\\end{bmatrix} \\in (0, 1)^{k-1}\n$$\n$$\nb(y) = 1\n$$\n\n根据$\\eta$可得：\n$$\n\\phi_i = e^{\\eta_i} \\cdot \\phi_k\n$$\n又因为：\n$$\n\\sum_{i=1}^{k}\\phi_i=\\sum_{i=1}^k\\phi_ke^{\\eta_i}=1\n$$\n故而：\n$$\n\\phi_k = \\frac{1}{\\sum_{i=1}^ke^{\\eta_i}}=\\frac{1}{e^{\\eta_k}+\\sum_{i=1}^{k-1}e^{\\eta_i}} = \\frac{1}{1+\\sum_{i=1}^{k-1}e^{\\eta_i}}\n$$\n所以：\n$$\n\\begin{align}\n\\phi_i &= e^{\\eta_i} \\cdot \\phi_k \\\\\\\n&= \\frac{e^{\\eta_i}}{1 + \\sum_{j=1}^{k-1}e^{\\eta_j}} \\\\\\\n&= \\frac{e^{\\theta_i^Tx_i}}{1 + \\sum_{j=1}^{k-1}e^{\\theta_j^Tx_j}}\n\\end{align}\n$$\n上述函数，被称为『softmax』函数，这个函数的作用经常用于进行归一化。\n\n经过上述步骤，假设函数可以被写成如下形式：\n$$\nh_\\theta(x)=\n\\left[\n\\begin{array}{c}\n1\\lbrace y=1 \\rbrace  \\\\\\\n\\vdots \\\\\\\n1\\lbrace y=k-1 \\rbrace\n\\end{array} | x;\\theta\n\\right]=\n\\begin{bmatrix}\n\\phi_1\\\\\\\n\\vdots\\\\\\\n\\phi_{k-1}\n\\end{bmatrix}\n$$\n\n#### 回归\n\n在经过上述推导，当我们有一堆训练集（$(x^{(1)}, y^{(1)}), \\cdots, (x^{(m)}, y^{(m)})$）用于训练的时候，则可以进行极大似然估计：\n$$\nL(\\theta) = \\prod_{i=1}^mP(y^{(i)} | x^{(i)};\\theta) = \\prod_{i=1}^m\\prod_{j=1}^k\\phi_j^{1\\lbrace y^{(i)}=j \\rbrace }\n$$\n","tags":["指数分布族"],"categories":["广义线性模型"]},{"title":"6. 牛顿法","url":"/blog-cs229/2018/06/07/6. 牛顿法/","content":"\n\n\n> **牛顿法**（英语：Newton's method）又称为**牛顿-拉弗森方法**（英语：Newton-Raphson method），它是一种在实数域和复数域上近似求解方程的方法。方法使用函数$\\displaystyle f(x)$的[泰勒级数](https://zh.wikipedia.org/wiki/%E6%B3%B0%E5%8B%92%E7%BA%A7%E6%95%B0)的前面几项来寻找方程$\\displaystyle f(y)=0$的根。\n>\n> ——维基百科\n\n牛顿法可以通过迭代逼近的方法，求得函数$f(x)=0$的解。\n\n![](http://o6vut8vrh.bkt.clouddn.com/18-6-7/69557176.jpg)\n\n1. 先初始化某个点$x_0$，对该点求导数$f'(x_0)$，可以得到一条切线；\n2. 切线会和横轴再有一个交点$x_1$，然后再重复第一步；\n3. 直到$f(x_n)=0$\n\n通过一系列推导，我们可以得知：\n$$\nx_{i+1}-x_{i}=\\frac{f(x^{(i)})}{f'(x^{(i)})}\n$$\n于是，我们可以将牛顿法用于极大似然估计，也就是求$l(\\theta)$的最大值，可以看做是求$l'(\\theta)=0$的解。\n\n那么，每次迭代就可以写成：\n$$\n\\theta^{(t+1)}=\\theta^{(t)}-\\frac{l'(\\theta^{(t)})}{l''(\\theta^{(t)}}\n$$\n更一般地，可以写成：\n$$\n\\theta^{(t+1)}=\\theta^{(t)}-H^{-1}\\nabla_\\theta l\n$$\n其中，$H$是$l(\\theta)$的Hessian矩阵：\n$$\nH_{ij}=\\frac{\\partial^2l}{\\partial\\theta_i\\partial\\theta_j}\n$$\n但这个方法有个缺点，每次迭代的时候，都需要重新计算$H^{-1}$，虽然牛顿法对函数$f$有很多要求和限制，但对于logistic函数而言，足够有效。","categories":["牛顿法"]},{"title":"5. 二分类问题","url":"/blog-cs229/2018/06/05/5. 二分类问题/","content":"\n\n\n在二分类问题中，输出$y\\in \\{0, 1\\}$。同样的，我们也可以用线性拟合来尝试解决二分类问题（如下图左），但数据点比较异常时，容易出现下图右这样的情况：\n\n![](http://o6vut8vrh.bkt.clouddn.com/18-6-5/41455819.jpg)\n\n一般，在二分类问题中，我们会选用『*logistic*函数』来拟合（因为形状像*S*，又称为『*sigmoid*函数』）：\n$$\nh_\\theta (x)=g(\\theta^Tx)=\\frac{1}{1+e^{-\\theta^Tx}}\n$$\n*logistic*函数$g(z)=1/(1+e^{-z})​$的形状如下：\n![](http://o6vut8vrh.bkt.clouddn.com/18-6-5/53166407.jpg)\n可以定义\n$$\n\\begin{align}\nP(y=1|x;\\theta)& =h_\\theta (x) \\\\\\\nP(y=0|x;\\theta)& =1-h_\\theta(x)\n\\end{align}\n$$\n于是：\n$$\nP(y|x;\\theta)=h_\\theta(x)^y(1-h_\\theta(x))^{(1-y)}\n$$\n进行极大似然估计：\n$$\nL(\\theta)=P(y|x;\\theta)=\\prod_{i=1}^mP(y^{(i)}|x^{(i)};\\theta)=\\prod_{i=1}^mh_\\theta(x^{(i)})^{y^{(i)}}(1-h_\\theta(x^{(i)}))^{(1-y^{(i)})}\n$$\n为了计算方便，定义\n$$\n\\begin{align}\nl(\\theta)&=log(L(\\theta))\\\\\\\n&=\\sum_{i=1}^mlog(P(y^{(i)}|x^{(i)};\\theta))\\\\\\\n&=\\sum_{i=1}^m(y^{(i)}\\cdot log(h_\\theta(x^{(i)}))+(1-y^{(i)})\\cdot log(1-h_\\theta(x^{(i)})))\n\\end{align}\n$$\n利用梯度上升进行求解：\n$$\n\\theta := \\theta + \\alpha \\nabla_\\theta l(\\theta)\n$$\n其中\n$$\n\\nabla_{\\theta_j} l(\\theta)=\\frac{\\partial}{\\partial\\theta_j}l(\\theta)=\\sum_{i=1}^m(y^{(i)}-h_\\theta(x^{(i)}))\\cdot x_j^{(i)}\\\\\\\n\\theta_j:=\\theta_j+\\alpha \\cdot \\sum_{i=1}^m(y^{(i)}-h_\\theta(x^{(i)}))\\cdot x_j^{(i)}\n$$\n最终的梯度上升结果几乎与线性拟合中的梯度下降结果一样。\n\n","tags":["二分类"],"categories":["分类"]},{"title":"4. 线性模型的概率解释","url":"/blog-cs229/2018/06/05/4. 线性模型的概率解释/","content":"\n关于：为何在进行线性回归时，选择用最小二乘拟合（距离的平方和）来进行，而不是选用其他的模型（比如三次方或四次方）？\n\n我们更新一下假设函数，使之变为：\n$$\ny^{(i)} = \\theta^Tx^{(i)} + \\varepsilon^{(i)}\n$$\n其中，$\\varepsilon^{(i)}$是误差项，表示未捕获的特征（unmodeled effects），比如房子存在壁炉也影响价格，或者其他的一些随机噪音（random noise）。\n\n一般，会假设误差项$\\varepsilon^{(i)} \\sim N(0, \\sigma^2)$（满足正态分布），也就是：\n$$\nP(\\varepsilon^{(i)})=\\frac{1}{\\sqrt{2\\pi}\\sigma}exp(-\\frac{(\\varepsilon^{(i)})^2}{2\\sigma^2})\n$$\n关于为什么假设正态分布的解释：\n\n1. 便于数学运算；\n2. 很多独立分布的变量之间相互叠加后会趋向于正态分布（中心极限定理），在大多数情况下能成立\n\n所以，$y^{(i)}$的后验分布：\n$$\nP(y^{(i)}|x^{(i)};\\theta)=\\frac{1}{\\sqrt{2\\pi}\\sigma}exp(-\\frac{(y^{(i)}-\\theta^Tx^{(i)})^2}{2\\sigma^2}) \\sim N(\\theta^Tx^{(i)}, \\sigma^2)\n$$\n\n之后，进行极大似然估计（maximum likelihood estimation）：$max L(\\theta)$，即选择合适的$\\theta$，使得$y^{(i)}$对于$x^{(i)}$出现的概率最高（有一些存在即合理的感觉），其中$L(\\theta)$的定义如下：\n$$\nL(\\theta)=P(y|x;\\theta)=\\prod_{i=1}^mP(y^{(i)}|x^{(i)};\\theta)=\\prod_{i=1}^m\\frac{1}{\\sqrt{2\\pi}\\sigma}exp(-\\frac{(y^{(i)}-\\theta^Tx^{(i)})^2}{2\\sigma^2})\n$$\n那么，为了计算方便，我们定义：\n$$\nl(\\theta) = log(L(\\theta))=\\sum_{i=1}^mlog(P(y^{(i)}|x^{(i)};\\theta))=m\\cdot log(\\frac{1}{\\sqrt{2\\pi}\\sigma})-\\sum_{i=1}^m\\frac{(y^{(i)}-\\theta^Tx^{(i)})^2}{2\\sigma^2}\n$$\n于是，极大似然估计变为最小化：\n$$\n\\sum_{i=1}^m\\frac{(y{(i)}-\\theta^Tx{(i)})2}{2\\sigma2}\n$$\n也即之前线性回归所需进行最小二乘拟合的$J(\\theta)$。","tags":["线性回归"],"categories":["线性回归"]},{"title":"3. 过拟合&局部加权回归","url":"/blog-cs229/2018/06/04/3. 过拟合&局部加权回归/","content":"\n\n\n## 欠拟合和过拟合\n\n对于之前房价的例子，假设只有一个特征size。\n\n假如，我们只用简单的线性拟合（$\\theta_0+\\theta_1x_1$，$x_1$表示size），最终拟合结果会变一条直线，就可能产生下图最左边的结果，我们称之为『欠拟合』。\n\n当我们尝试用二次曲线来拟合（$\\theta_0+\\theta_1x_1+\\theta_2x_1^2$，可以假设$x_2=x_1^2$，再进行线性拟合），就可能产生中间的结果。\n\n但如果再继续增加曲线的复杂度，对于下图这种五个样本的例子，假如我们用一个五次曲线来拟合它（$\\theta_0+\\theta_1x1+\\theta_2x1^2+\\cdots+\\theta_5x_1^5$）就会精确拟合所有数据，产生右图的结果，我们称之为『过拟合』。\n\n![](http://o6vut8vrh.bkt.clouddn.com/18-6-4/71073333.jpg)\n\n\n\n## 局部加权回归（Locally Weighted Regression）\n\n局部加权回归，是一种特定的非参数学习方法。\n\n什么叫非参数学习方法，首先，简单了解一下『参数化学习方法』(parametric learning algorithm)，是一种参数固定的学习方法，如上所示。而『非参数化学习方法』（non-parametric learning algorithm）则不固定参数，参数的个数会随着训练集数量而增长。\n\n我们回顾一下，线性拟合中，我们的目标是找到合适的参数$\\theta$，使得最小化$\\sum_i(Y^{(i)} - \\theta^TX^{(i)})^2$。\n\n而『局部线性拟合』，则是在某个局部区域A进行线性拟合，目标是最小化$\\sum_iw^{(i)}(Y^{(i)} - \\theta^TX^{(i)})^2$，其中权重$$w^{(i)} = exp(-\\frac{(x^{(i))}-x)^2}{2})$$，当然，权重公式是可替换的。\n\n![](http://o6vut8vrh.bkt.clouddn.com/18-6-4/30285296.jpg)\n\n我们观察一下$w^{(i)}$的形状，当数据$x^{(i)}$靠近$x$时，其权重就会较大，那么对目标函数的贡献就会大一些；而数据远离$x$的时候，权重就会较小，贡献就会较小。这样做，目标函数就会更关注$x$附近的数据点，从而达到局部的目的。\n\n![](http://o6vut8vrh.bkt.clouddn.com/18-6-4/78196882.jpg)\n\n当然，可以调整权重函数，常用的另一个权重函数：$$w^{(i)} = exp(-\\frac{(x^{(i))}-x)^2}{2 \\tau^2 })$$（波长函数），$\\tau$越大，波形越平缓，局部性越差。\n\n![](http://o6vut8vrh.bkt.clouddn.com/18-6-4/54967961.jpg)\n\n但问题在于，当训练数据较大时，该方法的代价会很高。每要预测一个值，就需要重新进行一次局部线性拟合。\n\n\n\n","tags":["课堂笔记"],"categories":["课堂笔记"]},{"title":"2. 线性回归","url":"/blog-cs229/2018/06/03/2. 线性回归/","content":"\n首先引入一些后面会用到的定理：\n\n**定义1**：定义函数$f: \\Bbb R^{m \\times n} \\mapsto \\Bbb R$，$A \\in \\Bbb R^{m \\times n}$，定义\n$$\n\\nabla_Af(A)=\n    \\begin{bmatrix}\n    \\frac{\\partial f}{\\partial A_{11}} & \\cdots & \\frac{\\partial f}{\\partial A_{1n}}\\\\\\\n    \\vdots & \\ddots & \\vdots \\\\\\\n    \\frac{\\partial f}{\\partial A_{m1}} & \\cdots & \\frac{\\partial f}{\\partial A_{mn}}\n    \\end{bmatrix}\n$$\n**定义2**：矩阵的迹（Trace）：如果$A \\in R^{n\\times n}$方阵，那么$A$的迹，是$A$对角线元素之和\n$$\ntr A = \\sum_{i=1}^nA_{ii}\n$$\n**定理1**：$tr AB = tr BA$\n\n**定理2**：$tr ABC = tr CAB = tr BCA$\n\n**定理3**：$f(A)=tr AB \\Rightarrow \\nabla_Af(A)=B^T$\n\n**定理4**：$trA = tr A^T$\n\n**定理5**：$a \\in R \\Rightarrow tr a=a$\n\n**定理6**：$\\nabla_AtrABA^TC=CAB+C^TAB^T$\n\n## 线性回归\n\n#### 一些符号的改写\n\n[上一篇博客](http://jackieanxis.coding.me/2018/06/03/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95/)提到，梯度下降的每一步，对某个参数$\\theta_i$，执行：\n$$\n\\displaystyle \\theta_i:=\\theta_i - \\alpha\\frac{\\partial}{\\partial \\theta_i}J(\\theta)\n$$\n那么，$h_\\theta(x)$的所有参数$\\theta$可以表示成一列向量：\n$$\n\\theta = \\left[\n\t\\begin{array}{c}\n\t\\theta_0\\\\\\\n\t\\theta_1\\\\\\\n\t\\vdots\\\\\\\n\t\\theta_n\n\t\\end{array}\n\\right] \\in R^{n+1}\n$$\n\n\n我们可以定义：\n$$\n\\nabla_\\theta J = \\left[\n\t\\begin{array}{c}\n\t\\frac{\\partial}{\\partial \\theta_0}J\\\\\\\n\t\\frac{\\partial}{\\partial \\theta_1}J\\\\\\\n\t\\vdots\\\\\\\n\t\\frac{\\partial}{\\partial \\theta_n}J\n\t\\end{array}\n\\right] \\in R^{n+1}\n$$\n\n梯度下降过程可以表示成：\n$$\n\\theta:=\\theta - \\alpha\\nabla_\\theta J\n$$\n其中，$\\theta$和$\\nabla_\\theta J$都说是n+1维向量。\n\n对于训练集中所有的输入${x^{(1)}},x^{(2)},…,x^{(m)}$，其中\n$$\nx^{(i)} = \\left[\n\t\\begin{array}{c}\n\t1\\\\\\\n\tx_1^{(i)}\\\\\\\n\t\\vdots\\\\\\\n\tx_n^{(i)}\\\\\\\n\t\\end{array}\n\\right] \\in R^{n+1}\n$$\n\n\n$h(x)=h_{\\theta}(x)=\\theta_0+\\theta_1x_1+\\theta_2x_2+...+\\theta_nx_n$，可以表示成向量：\n$$\n\\left[\n    \\begin{array}{c}\n    h_\\theta(x^{(1)})\\\\\\\n    h_\\theta(x^{(2)})\\\\\\\n    \\vdots\\\\\\\n    h_\\theta(x^{(m)})\\\\\\\n    \\end{array}\n\\right] = \\left[\n    \\begin{array}{c}\n    (x^{(1)})^T\\theta\\\\\\\n    (x^{(2)})^T\\theta\\\\\\\n    \\vdots\\\\\\\n    (x^{(m)})^T\\theta\\\\\\\n    \\end{array}\n\\right] = \\left[\n\t\\begin{array}{c}\n\t(x^{(1)})^T\\\\\\\n\t(x^{(2)})^T\\\\\\\n\t\\vdots\\\\\\\n\t(x^{(m)})^T\n\t\\end{array}\n\\right] \\cdot \\theta = X \\cdot \\theta\n$$\n\n而\n$$\nY = \\left[\n\t\\begin{array}{c}\n\ty^{(1)}\\\\\\\n\ty^{(2)}\\\\\\\n\t\\vdots\\\\\\\n\ty^{(m)}\n\t\\end{array}\n\\right]\n$$\n于是，\n$$\nJ(\\theta) = \\frac{1}{2}\\sum_{i=1}^{m}(h(x^{(i)} - y^{(i)})^2)=\\frac{1}{2}(X \\cdot \\theta - Y)^T(X \\cdot \\theta - Y)\n$$\n\n\n#### 推导过程\n\n关于梯度下降法，可以直接简化为求梯度为0的位置，即求$\\nabla_\\theta J(\\theta) = \\vec{0}$\n\n首先，简化：\n$$\n\\begin{align}\n\\nabla_\\theta J(\\theta) & = \\nabla_\\theta\\frac{1}{2}(X \\cdot \\theta - Y)^T(X \\cdot \\theta - Y)\\\\\\\n& =\\frac{1}{2}\\nabla_\\theta tr(\\theta^TX^TX\\theta - \\theta^TX^TY - Y^TX\\theta + Y^TY)\\\\\\\n& =\\frac{1}{2}[\\nabla_\\theta tr(\\theta\\theta^TX^TX) - \\nabla_\\theta tr(Y^TX\\theta) - \\nabla tr(Y^TX\\theta)]\n\\end{align}\n$$\n其中，第一项：\n$$\n\\begin{align}\n\\nabla_\\theta tr(\\theta\\theta^TX^TX) & = \\nabla_\\theta tr(\\theta I \\theta^TX^TX) &\\text{定理6, set: $\\theta =^{set} A, I = B, X^TX=C$}\\\\\\\n& = X^TX\\theta I + X^TX\\theta I & \\text{$CAB+C^TAB^T$}\\\\\\\n& = X^TX\\theta + X^TX\\theta\n\\end{align}\n$$\n第二项和第三项：\n$$\n\\nabla_\\theta tr(Y^TX\\theta) = X^TY\\\\\\\n(定理3，set:Y^TX = B, \\theta = A)\n$$\n所以：\n$$\n\\nabla_\\theta J(\\theta) = X^TX\\theta - X^TY = 0\\\\\\\n\\Rightarrow X^TX\\theta = X^TY\\\\\\\n$$\n最后解得：\n$$\n\\theta = (X^TX)^{(-1)}X^TY\n$$\n当然，以上的解是有限制的，只有当$X^TX$满秩时，才能够求逆。\n\n如果非满秩，说明方程数量不够，也就是当需要n个参数时，却不够n个输入样本。","tags":["课堂笔记"],"categories":["课堂笔记"]},{"title":"1. 监督学习&梯度下降法","url":"/blog-cs229/2018/06/03/1. 监督学习&梯度下降法/","content":"\n## 监督学习\n\n### 符号定义：\n\n| 符号                 | 意义                     |\n| -------------------- | ------------------------ |\n| $m$                  | 训练集包含的数据个数     |\n| $x$                  | 输入变量/特征（\u0004feature） |\n| $y$                  | 输出变量/目标（target）  |\n| $(x, y)$             | 一个训连样本             |\n| $(x^{(i)}, y^{(i)})$ | 第i个训练样本            |\n\n\n\n### 监督学习的主要流程：\n\n![](http://o6vut8vrh.bkt.clouddn.com/18-6-3/4328727.jpg)\n\n\n\n### 线性回归\n\n以预测房价为例，我们的目标是导出一个函数（即假设），根据房子的特征（比如大小、卧室数量等等）来预测房价，那么：\n- 输入（特征）：$x_1, x_2, …$（比如大小、卧室数量等等）\n- 输出（目标）：$y$（房价）\n- 假设：$h(x)=h_{\\theta}(x)=\\theta_0+\\theta_1x_1+\\theta_2x_2+...+\\theta_nx_n$，用于预测房价，其中$\\theta_i$是参数，$n$是特征数量\n\n  为了方便，可以将假设写成：$h(x)=\\sum_{i=0}^n\\theta_ix_i=\\theta^Tx​$\n\n此时，学习函数（Learning Algorithm）的目标就是找到合适的参数$\\theta$，使之能够导出『合理』的假设$h(x)$，这里我们将『合理』理解为：$h_\\theta(x)$（假设）和$y$（目标）之间的差距最小，也即：\n\n$$\n\\displaystyle \\min_{\\theta}\\frac{1}{2}\\sum_{i=1}^m(h_\\theta(x)^{(i)}-y_{(i)})^2\n$$\n这里的$\\frac{1}{2}$是为了简化之后的计算。\n\n我们定义$$\\displaystyle J(\\theta)=\\frac{1}{2}\\sum_{i=1}^m(h_\\theta(x)^{(i)}-y_{(i)})^2$$，那么我们的目标就是去选取合适的$\\theta$，以最小化$J(\\theta)$。\n\n\n\n## 梯度下降法\n\n### 搜索算法（梯度下降）\n\n目的：不断改变$\\theta$，从而来减少$J(\\theta)$。\n\n原理：每次都往下降最快的地方走，从而找到一个局部最优解。\n\n![](http://o6vut8vrh.bkt.clouddn.com/18-6-3/2339619.jpg)\n\n一般会初始化$\\vec{\\theta}=\\vec{0}$，然后每次都沿着梯度方向走，以保证每次都往下降最快的地方走：\n$$\n\\displaystyle \\theta_i:=\\theta_i - \\alpha\\frac{\\partial}{\\partial \\theta_i}J(\\theta)\n$$\n其中，$:=$表示赋值操作，$\\alpha$为步长。\n\n对于某个训练样本$(x, y)$\n$$\\displaystyle \\frac{\\partial}{\\partial \\theta_i}J(\\theta) = \\frac{\\partial}{\\partial \\theta_i}(\\frac{1}{2}(h_\\theta(x)-y)^2)$$\n\t$$\\displaystyle = 2 \\times \\frac{1}{2}(h_\\theta(x)-y)\\frac{\\partial}{\\partial \\theta_i}(h_\\theta(x)-y)$$\n\t$$\\displaystyle = (h_\\theta(x)-y)\\frac{\\partial}{\\partial \\theta_i}(\\theta_0x_0+…+\\theta_nx_n-y)$$\n\t$$\\displaystyle =(h_\\theta(x)-y) \\times x_i$$\n\n那么，$\\theta_i:=\\theta_i - \\alpha (h_\\theta(x)-y) \\times x_i$\n\n\n\n### 批量梯度下降法（Batch Gradient Descent）\n\n批量梯度下降法，使用的是所有训练样本的平均梯度：\n\n$$\n\\displaystyle \\theta_i:=\\theta_i - \\alpha \\frac{1}{m} \\sum_{j=1}^m(h_\\theta(x^{(j)})-y^{(j)}) \\times x_i^{(j)}\n$$\n\n但每次下降都需要遍历所有样本，效率较低，具体过程可能如下：\n\n![](http://o6vut8vrh.bkt.clouddn.com/18-6-3/49403960.jpg)\n\n### 随机梯度下降法（Stochastic Gradient Descent）\n\n又称为『增量梯度下降法』\n\n对每个样本$(x_{(j)}, y_{(j)})$进行：\n$$\n\\displaystyle \\theta_i:=\\theta_i - \\alpha (h_\\theta(x^{(j)})-y^{(j)}) \\times x_i^{(j)}\n$$\n\n直到收敛\n\n这时，每次梯度下降只遍历一个样本，具体过程可能如下：\n\n![](http://o6vut8vrh.bkt.clouddn.com/18-6-3/85884341.jpg)","tags":["线性回归"],"categories":["梯度下降"]}]