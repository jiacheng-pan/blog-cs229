[{"title":"二分类问题","url":"/blog-cs229/2018/06/05/二分类问题/","content":"\n\n\n在二分类问题中，输出$y\\in \\{0, 1\\}$。同样的，我们也可以用线性拟合来尝试解决二分类问题（如下图左），但数据点比较异常时，容易出现下图右这样的情况：\n\n![](http://o6vut8vrh.bkt.clouddn.com/18-6-5/41455819.jpg)\n\n一般，在二分类问题中，我们会选用『logistic函数』来拟合（因为形状像S，又称为『sigmod函数』）：\n$$\nh_\\theta (x)=g(\\theta^Tx)=\\frac{1}{1+e^{-\\theta^Tx}}\n$$\nlogistic函数$g(z)=1/(1+e^{-z})$的形状如下：\n![](http://o6vut8vrh.bkt.clouddn.com/18-6-5/53166407.jpg)\n可以定义\n$$\n\\begin{align}\nP(y=1|x;\\theta)& =h\\theta (x) \\\\\\\nP(y=0|x;\\theta)& =1-h\\theta(x)\n\\end{align}\n$$\n于是：\n$$\nP(y|x;\\theta)=h_\\theta(x)^y(1-h_\\theta(x))^{(1-y)}\n$$\n进行极大似然估计：\n$$\nL(\\theta)=P(y|x;\\theta)=\\prod_{i=1}^mP(y^{(i)}|x^{(i)};\\theta)=\\prod_{i=1}^mh_\\theta(x^{(i)})^{y^{(i)}}(1-h_\\theta(x^{(i)}))^{(1-y^{(i)})}\n$$\n为了计算方便，定义\n$$\n\\begin{align}\nl(\\theta)&=log(L(\\theta))\\\\\\\n&=\\sum_{i=1}^mlog(P(y^{(i)}|x^{(i)};\\theta))\\\\\\\n&=\\sum_{i=1}^m(y^{(i)}\\cdot log(h_\\theta(x^{(i)}))+(1-y^{(i)})\\cdot log(1-h_\\theta(x^{(i)})))\n\\end{align}\n$$\n利用梯度上升进行求解：\n$$\n\\theta := \\theta + \\alpha \\nabla_\\theta l(\\theta)\n$$\n其中\n$$\n\\nabla_{\\theta_j} l(\\theta)=\\frac{\\partial}{\\partial\\theta_j}l(\\theta)=\\sum_{i=1}^m(y^{(i)}-h_\\theta(x^{(i)}))\\cdot x_j^{(i)}\\\\\\\n\\theta_j:=\\theta_j+\\alpha \\cdot \\sum_{i=1}^m(y^{(i)}-h_\\theta(x^{(i)}))\\cdot x_j^{(i)}\n$$\n最终的梯度上升结果几乎与线性拟合中的梯度下降结果一样。\n\n","tags":["二分类"],"categories":["分类"]},{"title":"线性模型的概率解释","url":"/blog-cs229/2018/06/05/线性模型的概率解释/","content":"\n关于：为何在进行线性回归时，选择用最小二乘拟合（距离的平方和）来进行，而不是选用其他的模型（比如三次方或四次方）？\n\n我们更新一下假设函数，使之变为：\n$$\ny^{(i)} = \\theta^Tx^{(i)} + \\varepsilon^{(i)}\n$$\n其中，$\\varepsilon^{(i)}$是误差项，表示未捕获的特征（unmodeled effects），比如房子存在壁炉也影响价格，或者其他的一些随机噪音（random noise）。\n\n一般，会假设误差项$\\varepsilon^{(i)} \\sim N(0, \\sigma^2)$（满足正态分布），也就是：\n$$\nP(\\varepsilon^{(i)})=\\frac{1}{\\sqrt{2\\pi}\\sigma}exp(-\\frac{(\\varepsilon^{(i)})^2}{2\\sigma^2})\n$$\n关于为什么假设正态分布的解释：\n\n1. 便于数学运算；\n2. 很多独立分布的变量之间相互叠加后会趋向于正态分布（中心极限定理），在大多数情况下能成立\n\n所以，$y^{(i)}$的后验分布：\n$$\nP(y^{(i)}|x^{(i)};\\theta)=\\frac{1}{\\sqrt{2\\pi}\\sigma}exp(-\\frac{(y^{(i)}-\\theta^Tx^{(i)})^2}{2\\sigma^2}) \\sim N(\\theta^Tx^{(i)}, \\sigma^2)\n$$\n\n之后，进行极大似然估计（maximum likelihood estimation）：$max L(\\theta)$，即选择合适的$\\theta$，使得$y^{(i)}$对于$x^{(i)}$出现的概率最高（有一些存在即合理的感觉），其中$L(\\theta)$的定义如下：\n$$\nL(\\theta)=P(y|x;\\theta)=\\prod_{i=1}^mP(y^{(i)}|x^{(i)};\\theta)=\\prod_{i=1}^m\\frac{1}{\\sqrt{2\\pi}\\sigma}exp(-\\frac{(y^{(i)}-\\theta^Tx^{(i)})^2}{2\\sigma^2})\n$$\n那么，为了计算方便，我们定义：\n$$\nl(\\theta) = log(L(\\theta))=\\sum_{i=1}^mlog(P(y^{(i)}|x^{(i)};\\theta))=m\\cdot log(\\frac{1}{\\sqrt{2\\pi}\\sigma})-\\sum_{i=1}^m\\frac{(y^{(i)}-\\theta^Tx^{(i)})^2}{2\\sigma^2}\n$$\n于是，极大似然估计变为最小化：\n$$\n\\sum_{i=1}^m\\frac{(y{(i)}-\\theta^Tx{(i)})2}{2\\sigma2}\n$$\n也即之前线性回归所需进行最小二乘拟合的$J(\\theta)$。","tags":["线性回归"],"categories":["线性回归"]},{"title":"过拟合&局部加权回归","url":"/blog-cs229/2018/06/04/过拟合&局部加权回归/","content":"\n\n\n## 欠拟合和过拟合\n\n对于之前房价的例子，假设只有一个特征size。\n\n假如，我们只用简单的线性拟合（$\\theta_0+\\theta_1x_1$，$x_1$表示size），最终拟合结果会变一条直线，就可能产生下图最左边的结果，我们称之为『欠拟合』。\n\n当我们尝试用二次曲线来拟合（$\\theta_0+\\theta_1x_1+\\theta_2x_1^2$，可以假设$x_2=x_1^2$，再进行线性拟合），就可能产生中间的结果。\n\n但如果再继续增加曲线的复杂度，对于下图这种五个样本的例子，假如我们用一个五次曲线来拟合它（$\\theta_0+\\theta_1x1+\\theta_2x1^2+\\cdots+\\theta_5x_1^5$）就会精确拟合所有数据，产生右图的结果，我们称之为『过拟合』。\n\n![](http://o6vut8vrh.bkt.clouddn.com/18-6-4/71073333.jpg)\n\n\n\n## 局部加权回归（Locally Weighted Regression）\n\n局部加权回归，是一种特定的非参数学习方法。\n\n什么叫非参数学习方法，首先，简单了解一下『参数化学习方法』(parametric learning algorithm)，是一种参数固定的学习方法，如上所示。而『非参数化学习方法』（non-parametric learning algorithm）则不固定参数，参数的个数会随着训练集数量而增长。\n\n我们回顾一下，线性拟合中，我们的目标是找到合适的参数$\\theta$，使得最小化$\\sum_i(Y^{(i)} - \\theta^TX^{(i)})^2$。\n\n而『局部线性拟合』，则是在某个局部区域A进行线性拟合，目标是最小化$\\sum_iw^{(i)}(Y^{(i)} - \\theta^TX^{(i)})^2$，其中权重$$w^{(i)} = exp(-\\frac{(x^{(i))}-x)^2}{2})$$，当然，权重公式是可替换的。\n\n![](http://o6vut8vrh.bkt.clouddn.com/18-6-4/30285296.jpg)\n\n我们观察一下$w^{(i)}$的形状，当数据$x^{(i)}$靠近$x$时，其权重就会较大，那么对目标函数的贡献就会大一些；而数据远离$x$的时候，权重就会较小，贡献就会较小。这样做，目标函数就会更关注$x$附近的数据点，从而达到局部的目的。\n\n![](http://o6vut8vrh.bkt.clouddn.com/18-6-4/78196882.jpg)\n\n当然，可以调整权重函数，常用的另一个权重函数：$$w^{(i)} = exp(-\\frac{(x^{(i))}-x)^2}{2 \\tau^2 })$$（波长函数），$\\tau$越大，波形越平缓，局部性越差。\n\n![](http://o6vut8vrh.bkt.clouddn.com/18-6-4/54967961.jpg)\n\n但问题在于，当训练数据较大时，该方法的代价会很高。每要预测一个值，就需要重新进行一次局部线性拟合。\n\n\n\n","tags":["课堂笔记"],"categories":["课堂笔记"]},{"title":"线性回归","url":"/blog-cs229/2018/06/03/线性回归&最小二乘拟合/","content":"\n首先引入一些后面会用到的定理：\n\n**定义1**：定义函数$f: \\Bbb R^{m \\times n} \\mapsto \\Bbb R$，$A \\in \\Bbb R^{m \\times n}$，定义\n$$\n\\nabla_Af(A)=\n    \\begin{bmatrix}\n    \\frac{\\partial f}{\\partial A_{11}} & \\cdots & \\frac{\\partial f}{\\partial A_{1n}}\\\\\\\n    \\vdots & \\ddots & \\vdots \\\\\\\n    \\frac{\\partial f}{\\partial A_{m1}} & \\cdots & \\frac{\\partial f}{\\partial A_{mn}}\n    \\end{bmatrix}\n$$\n**定义2**：矩阵的迹（Trace）：如果$A \\in R^{n\\times n}$方阵，那么$A$的迹，是$A$对角线元素之和\n$$\ntr A = \\sum_{i=1}^nA_{ii}\n$$\n**定理1**：$tr AB = tr BA$\n\n**定理2**：$tr ABC = tr CAB = tr BCA$\n\n**定理3**：$f(A)=tr AB \\Rightarrow \\nabla_Af(A)=B^T$\n\n**定理4**：$trA = tr A^T$\n\n**定理5**：$a \\in R \\Rightarrow tr a=a$\n\n**定理6**：$\\nabla_AtrABA^TC=CAB+C^TAB^T$\n\n## 线性回归\n\n#### 一些符号的改写\n\n[上一篇博客](http://jackieanxis.coding.me/2018/06/03/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95/)提到，梯度下降的每一步，对某个参数$\\theta_i$，执行：\n$$\n\\displaystyle \\theta_i:=\\theta_i - \\alpha\\frac{\\partial}{\\partial \\theta_i}J(\\theta)\n$$\n那么，$h_\\theta(x)$的所有参数$\\theta$可以表示成一列向量：\n$$\n\\theta = \\left[\n\t\\begin{array}{c}\n\t\\theta_0\\\\\\\n\t\\theta_1\\\\\\\n\t\\vdots\\\\\\\n\t\\theta_n\n\t\\end{array}\n\\right] \\in R^{n+1}\n$$\n\n\n我们可以定义：\n$$\n\\nabla_\\theta J = \\left[\n\t\\begin{array}{c}\n\t\\frac{\\partial}{\\partial \\theta_0}J\\\\\\\n\t\\frac{\\partial}{\\partial \\theta_1}J\\\\\\\n\t\\vdots\\\\\\\n\t\\frac{\\partial}{\\partial \\theta_n}J\n\t\\end{array}\n\\right] \\in R^{n+1}\n$$\n\n梯度下降过程可以表示成：\n$$\n\\theta:=\\theta - \\alpha\\nabla_\\theta J\n$$\n其中，$\\theta$和$\\nabla_\\theta J$都说是n+1维向量。\n\n对于训练集中所有的输入${x^{(1)}},x^{(2)},…,x^{(m)}$，其中\n$$\nx^{(i)} = \\left[\n\t\\begin{array}{c}\n\t1\\\\\\\n\tx_1^{(i)}\\\\\\\n\t\\vdots\\\\\\\n\tx_n^{(i)}\\\\\\\n\t\\end{array}\n\\right] \\in R^{n+1}\n$$\n\n\n$h(x)=h_{\\theta}(x)=\\theta_0+\\theta_1x_1+\\theta_2x_2+...+\\theta_nx_n$，可以表示成向量：\n$$\n\\left[\n    \\begin{array}{c}\n    h_\\theta(x^{(1)})\\\\\\\n    h_\\theta(x^{(2)})\\\\\\\n    \\vdots\\\\\\\n    h_\\theta(x^{(m)})\\\\\\\n    \\end{array}\n\\right] = \\left[\n    \\begin{array}{c}\n    (x^{(1)})^T\\theta\\\\\\\n    (x^{(2)})^T\\theta\\\\\\\n    \\vdots\\\\\\\n    (x^{(m)})^T\\theta\\\\\\\n    \\end{array}\n\\right] = \\left[\n\t\\begin{array}{c}\n\t(x^{(1)})^T\\\\\\\n\t(x^{(2)})^T\\\\\\\n\t\\vdots\\\\\\\n\t(x^{(m)})^T\n\t\\end{array}\n\\right] \\cdot \\theta = X \\cdot \\theta\n$$\n\n而\n$$\nY = \\left[\n\t\\begin{array}{c}\n\ty^{(1)}\\\\\\\n\ty^{(2)}\\\\\\\n\t\\vdots\\\\\\\n\ty^{(m)}\n\t\\end{array}\n\\right]\n$$\n于是，\n$$\nJ(\\theta) = \\frac{1}{2}\\sum_{i=1}^{m}(h(x^{(i)} - y^{(i)})^2)=\\frac{1}{2}(X \\cdot \\theta - Y)^T(X \\cdot \\theta - Y)\n$$\n\n\n#### 推导过程\n\n关于梯度下降法，可以直接简化为求梯度为0的位置，即求$\\nabla_\\theta J(\\theta) = \\vec{0}$\n\n首先，简化：\n$$\n\\begin{align}\n\\nabla_\\theta J(\\theta) & = \\nabla_\\theta\\frac{1}{2}(X \\cdot \\theta - Y)^T(X \\cdot \\theta - Y)\\\\\\\n& =\\frac{1}{2}\\nabla_\\theta tr(\\theta^TX^TX\\theta - \\theta^TX^TY - Y^TX\\theta + Y^TY)\\\\\\\n& =\\frac{1}{2}[\\nabla_\\theta tr(\\theta\\theta^TX^TX) - \\nabla_\\theta tr(Y^TX\\theta) - \\nabla tr(Y^TX\\theta)]\n\\end{align}\n$$\n其中，第一项：\n$$\n\\begin{align}\n\\nabla_\\theta tr(\\theta\\theta^TX^TX) & = \\nabla_\\theta tr(\\theta I \\theta^TX^TX) &\\text{定理6, set: $\\theta =^{set} A, I = B, X^TX=C$}\\\\\\\n& = X^TX\\theta I + X^TX\\theta I & \\text{$CAB+C^TAB^T$}\\\\\\\n& = X^TX\\theta + X^TX\\theta\n\\end{align}\n$$\n第二项和第三项：\n$$\n\\nabla_\\theta tr(Y^TX\\theta) = X^TY\\\\\\\n(定理3，set:Y^TX = B, \\theta = A)\n$$\n所以：\n$$\n\\nabla_\\theta J(\\theta) = X^TX\\theta - X^TY = 0\\\\\\\n\\Rightarrow X^TX\\theta = X^TY\\\\\\\n$$\n最后解得：\n$$\n\\theta = (X^TX)^{(-1)}X^TY\n$$\n当然，以上的解是有限制的，只有当$X^TX$满秩时，才能够求逆。\n\n如果非满秩，说明方程数量不够，也就是当需要n个参数时，却不够n个输入样本。","tags":["课堂笔记"],"categories":["课堂笔记"]},{"title":"监督学习&梯度下降法","url":"/blog-cs229/2018/06/03/监督学习&梯度下降法/","content":"\n## 监督学习\n\n### 符号定义：\n\n| 符号                 | 意义                     |\n| -------------------- | ------------------------ |\n| $m$                  | 训练集包含的数据个数     |\n| $x$                  | 输入变量/特征（\u0004feature） |\n| $y$                  | 输出变量/目标（target）  |\n| $(x, y)$             | 一个训连样本             |\n| $(x^{(i)}, y^{(i)})$ | 第i个训练样本            |\n\n\n\n### 监督学习的主要流程：\n\n![](http://o6vut8vrh.bkt.clouddn.com/18-6-3/4328727.jpg)\n\n\n\n### 线性回归\n\n以预测房价为例，我们的目标是导出一个函数（即假设），根据房子的特征（比如大小、卧室数量等等）来预测房价，那么：\n- 输入（特征）：$x_1, x_2, …$（比如大小、卧室数量等等）\n- 输出（目标）：$y$（房价）\n- 假设：$h(x)=h_{\\theta}(x)=\\theta_0+\\theta_1x_1+\\theta_2x_2+...+\\theta_nx_n$，用于预测房价，其中$\\theta_i$是参数，$n$是特征数量\n\n  为了方便，可以将假设写成：$h(x)=\\sum_{i=0}^n\\theta_ix_i=\\theta^Tx​$\n\n此时，学习函数（Learning Algorithm）的目标就是找到合适的参数$\\theta$，使之能够导出『合理』的假设$h(x)$，这里我们将『合理』理解为：$h_\\theta(x)$（假设）和$y$（目标）之间的差距最小，也即：\n\n$$\n\\displaystyle \\min_{\\theta}\\frac{1}{2}\\sum_{i=1}^m(h_\\theta(x)^{(i)}-y_{(i)})^2\n$$\n这里的$\\frac{1}{2}$是为了简化之后的计算。\n\n我们定义$$\\displaystyle J(\\theta)=\\frac{1}{2}\\sum_{i=1}^m(h_\\theta(x)^{(i)}-y_{(i)})^2$$，那么我们的目标就是去选取合适的$\\theta$，以最小化$J(\\theta)$。\n\n\n\n## 梯度下降法\n\n### 搜索算法（梯度下降）\n\n目的：不断改变$\\theta$，从而来减少$J(\\theta)$。\n\n原理：每次都往下降最快的地方走，从而找到一个局部最优解。\n\n![](http://o6vut8vrh.bkt.clouddn.com/18-6-3/2339619.jpg)\n\n一般会初始化$\\vec{\\theta}=\\vec{0}$，然后每次都沿着梯度方向走，以保证每次都往下降最快的地方走：\n$$\n\\displaystyle \\theta_i:=\\theta_i - \\alpha\\frac{\\partial}{\\partial \\theta_i}J(\\theta)\n$$\n其中，$:=$表示赋值操作，$\\alpha$为步长。\n\n对于某个训练样本$(x, y)$\n$$\\displaystyle \\frac{\\partial}{\\partial \\theta_i}J(\\theta) = \\frac{\\partial}{\\partial \\theta_i}(\\frac{1}{2}(h_\\theta(x)-y)^2)$$\n\t$$\\displaystyle = 2 \\times \\frac{1}{2}(h_\\theta(x)-y)\\frac{\\partial}{\\partial \\theta_i}(h_\\theta(x)-y)$$\n\t$$\\displaystyle = (h_\\theta(x)-y)\\frac{\\partial}{\\partial \\theta_i}(\\theta_0x_0+…+\\theta_nx_n-y)$$\n\t$$\\displaystyle =(h_\\theta(x)-y) \\times x_i$$\n\n那么，$\\theta_i:=\\theta_i - \\alpha (h_\\theta(x)-y) \\times x_i$\n\n\n\n### 批量梯度下降法（Batch Gradient Descent）\n\n批量梯度下降法，使用的是所有训练样本的平均梯度：\n\n$$\n\\displaystyle \\theta_i:=\\theta_i - \\alpha \\frac{1}{m} \\sum_{j=1}^m(h_\\theta(x^{(j)})-y^{(j)}) \\times x_i^{(j)}\n$$\n\n但每次下降都需要遍历所有样本，效率较低，具体过程可能如下：\n\n![](http://o6vut8vrh.bkt.clouddn.com/18-6-3/49403960.jpg)\n\n### 随机梯度下降法（Stochastic Gradient Descent）\n\n又称为『增量梯度下降法』\n\n对每个样本$(x_{(j)}, y_{(j)})$进行：\n$$\n\\displaystyle \\theta_i:=\\theta_i - \\alpha (h_\\theta(x^{(j)})-y^{(j)}) \\times x_i^{(j)}\n$$\n\n直到收敛\n\n这时，每次梯度下降只遍历一个样本，具体过程可能如下：\n\n![](http://o6vut8vrh.bkt.clouddn.com/18-6-3/85884341.jpg)","tags":["线性回归"],"categories":["梯度下降"]}]